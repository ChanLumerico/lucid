## âš™ï¸ Optimizerì™€ LR Scheduler ì‹œìŠ¤í…œ

Lucidì˜ Optimizerì™€ LR SchedulerëŠ” PyTorchë¥¼ ë²¤ì¹˜ë§ˆí‚¹í•´ ë™ì¼í•œ íë¦„ì„ ëª©í‘œë¡œ ì„¤ê³„ëë‹¤. í•µì‹¬ì€ **`Optimizer`ê°€ `nn.Parameter`ì—ë§Œ ì‘ë™**í•˜ê³ , **`LRScheduler`ê°€ Optimizerì˜ `param_groups`ë¥¼ ë™ì ìœ¼ë¡œ ê°±ì‹ **í•˜ëŠ” êµ¬ì¡°ë‹¤. ì´ ê¸€ì—ì„œëŠ” ê° ë² ì´ìŠ¤ í´ë˜ìŠ¤ì˜ ì‹œê·¸ë‹ˆì²˜ì™€ ë‚´ë¶€ íë¦„, `nn.Module`/`nn.Parameter`ì™€ì˜ ìƒí˜¸ì‘ìš©, ê·¸ë¦¬ê³  ì‹¤ì œ ì‚¬ìš© ì˜ˆì œë¥¼ ì½”ë“œ ìŠ¤ë‹ˆí«ê³¼ í•¨ê»˜ ìƒì„¸íˆ í’€ì–´ë³¸ë‹¤.

---

### ğŸ§­ ì„¤ê³„ ëª©í‘œì™€ ì—­í•  ë¶„ë¦¬

- **Optimizer**: íŒŒë¼ë¯¸í„° ì§‘í•©(`nn.Parameter`)ì— ëŒ€í•´ step/zero_grad/state_dict/param_groups ê´€ë¦¬. íŒŒë¼ë¯¸í„° ì´ì™¸ì˜ TensorëŠ” ê±°ë¶€í•œë‹¤.
- **LRScheduler**: Optimizerì˜ `param_groups`ì— ì €ì¥ëœ í•™ìŠµë¥ ì„ ì‹œì (epoch/step)ì— ë”°ë¼ ê°±ì‹ . Optimizerì™€ ë…ë¦½ì ìœ¼ë¡œ ì €ì¥/ë¡œë“œ ê°€ëŠ¥.
- **í˜¸í™˜ì„±**: PyTorchì˜ API(closure, param_groups, state_dict, verbose ë“±)ì™€ ì‚¬ìš© íë¦„ì„ ìµœëŒ€í•œ ëª¨ì‚¬.

Lucidì˜ `Optimizer`ëŠ” íŒŒë¼ë¯¸í„°/ë²„í¼ ì‹œìŠ¤í…œ ìœ„ì— ì–¹í˜€ ìˆìœ¼ë©°, `nn.Module.parameters()`ê°€ ë°˜í™˜í•˜ëŠ” **Parameterë§Œ** ë°›ì•„ë“¤ì¸ë‹¤. `LRScheduler`ëŠ” Optimizerë¥¼ ì…ë ¥ë°›ì•„ í•™ìŠµë¥ ì„ ì¡°ì •í•˜ë˜, Optimizer ë‚´ë¶€ ìƒíƒœ(state_dict)ì™€ ë³„ë„ë¡œ ì§ë ¬í™”ëœë‹¤.

### ğŸ§± Optimizer ë² ì´ìŠ¤ í´ë˜ìŠ¤

- **ê²½ë¡œ**: [`lucid/optim/_base.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/optim/_base.py)

```python
class Optimizer(ABC):
    def __init__(self, params: Iterable[nn.Parameter], defaults: dict[str, Any]) -> None: ...

    @abstractmethod
    def step(self, closure: _OptimClosure | None = None) -> Any | None: ...

    def zero_grad(self) -> None: ...

    def param_groups_setup(self, params: list[nn.Parameter], defaults: dict[str, Any]) -> list[dict[str, Any]]: ...

    def add_param_group(self, param_group: dict[str, Any]) -> None: ...

    def state_dict(self) -> dict: ...

    def load_state_dict(self, state_dict: dict) -> None: ...
```

#### ì´ˆê¸°í™”ì™€ ê²€ì¦
- `params`ëŠ” **ë°˜ë“œì‹œ `nn.Parameter` ë°˜ë³µì**ì—¬ì•¼ í•˜ë©°, íƒ€ì… ê²€ì¦ í›„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³´ê´€.
- `defaults`ëŠ” í•™ìŠµë¥ , weight decay ë“± í•˜ìœ„ optimizerê°€ ê³µí†µìœ¼ë¡œ ì“°ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë‹´ëŠ”ë‹¤.
- `param_groups`: `param_groups_setup`ìœ¼ë¡œ ê·¸ë£¹í™”(ê¸°ë³¸ì€ ë‹¨ì¼ ê·¸ë£¹). ê·¸ë£¹ë§ˆë‹¤ `{"params": [...], **defaults}` í˜•íƒœ.
- `state`: `defaultdict(dict)`ë¡œ íŒŒë¼ë¯¸í„°ë³„ ìƒíƒœ ì €ì¥(ëª¨ë©˜í…€ ë²„í¼ ë“±), ì§ë ¬í™” ì‹œ ì¸ë±ìŠ¤ë¡œ ë§¤í•‘.

#### step/zero_grad
- `step(closure=None)`: í•˜ìœ„ í´ë˜ìŠ¤ê°€ êµ¬í˜„. í´ë¡œì €ëŠ” ì¬ê³„ì‚°ì´ í•„ìš”í•œ optimizer(SGD with line search ë“±) í˜¸í™˜ìš©.
- `zero_grad()`: ëª¨ë“  param_groupì˜ Parameter.gradë¥¼ 0ìœ¼ë¡œ ì„¤ì •. `nn.Parameter.zero_grad`ë¥¼ í˜¸ì¶œí•´ grad ëˆ„ì ì„ ì´ˆê¸°í™”.

#### param_group ê´€ë¦¬
- `add_param_group`: ìƒˆë¡œìš´ íŒŒë¼ë¯¸í„° ì„¸íŠ¸ë¥¼ ì¶”ê°€. ì¤‘ë³µ íŒŒë¼ë¯¸í„°ê°€ ì¡´ì¬í•˜ë©´ ì˜ˆì™¸ë¥¼ ë˜ì ¸ ë²„ê·¸ë¥¼ ë°©ì§€.
- ê·¸ë£¹ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë®ì–´ì“°ë˜, `params` í‚¤ë§Œ ë³„ë„ë¡œ ì·¨ê¸‰í•´ ë¦¬ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•œë‹¤.

#### state_dict ì§ë ¬í™”
```python
def state_dict(self) -> dict:
    param_to_idx = {p: i for i, p in enumerate(self._flat_params())}
    packed_state = {
        param_to_idx[p]: copy.deepcopy(st) 
        for p, st in self.state.items() if p in param_to_idx
    }
    packed_groups = [...]
    return {"state": packed_state, "param_groups": packed_groups}
```
- íŒŒë¼ë¯¸í„° ê°ì²´ ì°¸ì¡°ë¥¼ ì¸ë±ìŠ¤ë¡œ ì¹˜í™˜í•´ ì§ë ¬í™”. ë¡œë“œì‹œ í˜„ì¬ íŒŒë¼ë¯¸í„° ìˆœì„œì™€ ë§¤í•‘í•´ ìƒíƒœë¥¼ ë³µì›.
- param_groupë„ íŒŒë¼ë¯¸í„° ì¸ë±ìŠ¤ë¡œ ì €ì¥í•´ ì¬êµ¬ì„± ê°€ëŠ¥. PyTorch state_dict í¬ë§·ê³¼ ìœ ì‚¬.

### ğŸ”— Optimizer â†” `nn.Module`/`nn.Parameter`

- OptimizerëŠ” **Parameterë§Œ** ë°›ëŠ”ë‹¤. ë²„í¼ë‚˜ ì¼ë°˜ TensorëŠ” íƒ€ì… ì—ëŸ¬ë¥¼ ì¼ìœ¼í‚¨ë‹¤.
- ì¼ë°˜ì  íë¦„:
  1) `model = MyModule()`  
  2) `opt = OptimizerSubclass(model.parameters(), lr=...)`  
  3) `loss = criterion(model(x), y)`  
  4) `opt.zero_grad()` â†’ `loss.backward()` â†’ `opt.step()`
- `zero_grad()`ëŠ” `nn.Parameter.zero_grad()`ë¥¼ í˜¸ì¶œí•´ grad í•„ë“œë¥¼ ì´ˆê¸°í™”í•œë‹¤.
- param_groupsëŠ” ëª¨ë“ˆ íŠ¸ë¦¬ì™€ ë¬´ê´€í•˜ê²Œ **optimizer ë‚´ë¶€ì—ì„œ ê´€ë¦¬**ë˜ë¯€ë¡œ, íŠ¹ì • ë ˆì´ì–´ì— ë‹¤ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì ìš©í•˜ë ¤ë©´ ê·¸ë£¹ì„ ë¶„ë¦¬í•´ ì „ë‹¬í•œë‹¤.

#### ì˜ˆì‹œ: ì»¤ìŠ¤í…€ ê·¸ë£¹
```python
params = [
    {"params": model.backbone.parameters(), "lr": 1e-3},
    {"params": model.head.parameters(), "lr": 1e-2, "weight_decay": 1e-4},
]
opt = MyOptimizer(params, defaults={"lr": 1e-3, "weight_decay": 0.0})
```
`defaults`ëŠ” ê³µí†µê°’, ê·¸ë£¹ ë”•ì…”ë„ˆë¦¬ëŠ” íŠ¹ì • í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë®ì–´ì“´ë‹¤.

### ğŸ§­ LRScheduler ë² ì´ìŠ¤ í´ë˜ìŠ¤

- **ê²½ë¡œ**: [`lucid/optim/lr_scheduler/_base.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/optim/lr_scheduler/_base.py)

```python
class LRScheduler(ABC):
    def __init__(self, optimizer: Optimizer, last_epoch: int = -1, verbose: bool = False) -> None: ...

    @abstractmethod
    def get_lr(self) -> list[float]: ...

    def step(self, epoch: int | None = None) -> None: ...

    def state_dict(self) -> dict[str, Any]: ...

    def load_state_dict(self, state_dict: dict[str, Any]) -> None: ...

    @property
    def last_lr(self) -> list[float]: ...
```

#### ì´ˆê¸°í™”ì™€ ë² ì´ìŠ¤ ì†ì„±
- Optimizer ìœ íš¨ì„± ê²€ì¦: `param_groups`ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.
- `base_lrs`: ì´ˆê¸° ê° param_groupì˜ í•™ìŠµë¥ . ìŠ¤ì¼€ì¤„ ê³„ì‚°ì˜ ê¸°ì¤€.
- `last_epoch`: í˜„ì¬ê¹Œì§€ì˜ step/epoch ì¹´ìš´í„°. ì´ˆê¸°ê°’ -1ì€ `step()` í˜¸ì¶œ ì‹œ 0ë¶€í„° ì‹œì‘í•˜ë„ë¡ í•¨.
- `_last_lr`: ì§ì „ `step` ì´í›„ ì„¤ì •ëœ í•™ìŠµë¥  ê¸°ë¡.

#### stepê³¼ get_lr
```python
def step(self, epoch=None):
    if epoch is None: 
        self.last_epoch += 1
    else: 
        self.last_epoch = int(epoch)

    self._step_count += 1
    new_lrs = self.get_lr()
    ...
    for group, lr in zip(self.optimizer.param_groups, new_lrs):
        group["lr"] = float(lr)
    self._last_lr = [float(g["lr"]) for g in self.optimizer.param_groups]

    if self.verbose:
        print(f"Epoch {self.last_epoch}: setting learning rates to {self._last_lr}.")
```
- í•˜ìœ„ í´ë˜ìŠ¤ëŠ” **`get_lr`ë§Œ êµ¬í˜„**í•˜ë©´ ëœë‹¤. ë°˜í™˜ ë¦¬ìŠ¤íŠ¸ ê¸¸ì´ëŠ” param_groupsì™€ ê°™ì•„ì•¼ í•œë‹¤.
- `epoch` ì¸ìë¥¼ ì§ì ‘ ì£¼ë©´ ìŠ¤ì¼€ì¤„ì„ ì™¸ë¶€ ì¹´ìš´í„°ì™€ ë™ê¸°í™”í•  ìˆ˜ ìˆë‹¤.

#### state_dict
- `last_epoch`, `base_lrs`, `_step_count`, `_last_lr`, `_group_count`ë¥¼ ì €ì¥.
- ë¡œë“œì‹œ ê·¸ë£¹ ìˆ˜ê°€ ë‹¤ë¥´ë©´ ì—ëŸ¬ë¥¼ ë˜ì ¸ í˜¸í™˜ì„±ì„ ëª…í™•íˆ í•œë‹¤.

### ğŸ”„ Scheduler â†” Optimizer ìƒí˜¸ì‘ìš©

- SchedulerëŠ” Optimizer ë‚´ë¶€ì˜ `param_groups`ë¥¼ ì§ì ‘ ìˆ˜ì •í•´ í•™ìŠµë¥ ì„ ë°”ê¾¼ë‹¤. OptimizerëŠ” ì´ë¥¼ ì°¸ì¡°í•´ step ìˆ˜í–‰ ì‹œ ì‚¬ìš©.
- optimizer state_dictì™€ ë³„ê°œë¡œ ìŠ¤ì¼€ì¤„ëŸ¬ state_dictë¥¼ ì €ì¥/ë¡œë“œí•´ì•¼ ë™ì¼í•œ í•™ìŠµ ê³¡ì„ ì„ ì¬í˜„í•  ìˆ˜ ìˆë‹¤.
- ì¼ë°˜ì  íŒ¨í„´:
  ```python
  opt = MyOptimizer(model.parameters(), defaults={"lr": 1e-3})
  sched = MyScheduler(opt, ...)
  for epoch in range(num_epochs):
      for batch in data:
          ...
          loss.backward()
          opt.step()
          opt.zero_grad()
      sched.step()  # ë˜ëŠ” sched.step(epoch)
  ```
- ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ë¨¼ì €(step) ì ìš© í›„ optimizer stepì„ í˜¸ì¶œí•˜ëŠ” íŒ¨í„´, ê·¸ ë°˜ëŒ€ íŒ¨í„´ ë“±ì€ ìŠ¤ì¼€ì¤„ëŸ¬ êµ¬í˜„ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆì§€ë§Œ, Lucid ë² ì´ìŠ¤ëŠ” PyTorchì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ë¥¼ ë”°ë¥¸ë‹¤(ì‚¬ìš©ìê°€ ìˆœì„œë¥¼ ì •í•œë‹¤).

### ğŸ›  Optimizer/Scheduler state_dict ì‚¬ë¡€

- **Optimizer ì§ë ¬í™”**: íŒŒë¼ë¯¸í„°ë¥¼ ì¸ë±ìŠ¤ë¡œ ë§¤í•‘í•´ stateë¥¼ ì €ì¥. ëª¨ë©˜í…€/ì ì‘ì  í†µê³„ ë“± í•˜ìœ„ í´ë˜ìŠ¤ ìƒíƒœë¥¼ ê·¸ëŒ€ë¡œ í¬í•¨.
- **Scheduler ì§ë ¬í™”**: `last_epoch`ì™€ ìµœê·¼ í•™ìŠµë¥ , ìŠ¤í… ì¹´ìš´íŠ¸ë¥¼ ì €ì¥. param_group ìˆ˜ê°€ ë‹¤ë¥´ë©´ ë¡œë”© ì‹œ ì—ëŸ¬.
- **ë¡œë“œ ìˆœì„œ**: ì¼ë°˜ì ìœ¼ë¡œ Optimizerë¥¼ ë¨¼ì € ë¡œë“œí•˜ê³ , ë™ì¼ Optimizer ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°€ì§„ Schedulerë¥¼ ë¡œë“œí•œë‹¤. ì˜ˆ:
  ```python
  opt.load_state_dict(opt_state)
  sched = MyScheduler(opt, ...)
  sched.load_state_dict(sched_state)
  ```
- **ì£¼ì˜**: ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆœì„œê°€ ë°”ë€Œë©´ Optimizer/Scheduler state ë³µì›ì´ ì‹¤íŒ¨í•˜ê±°ë‚˜ ì˜ëª»ëœ íŒŒë¼ë¯¸í„°ì— ìƒíƒœê°€ ë§¤í•‘ë  ìˆ˜ ìˆë‹¤. model.state_dictì™€ í•¨ê»˜ ì €ì¥/ë¡œë“œ ìˆœì„œë¥¼ ìœ ì§€í•´ì•¼ í•œë‹¤.

---

### ğŸ“š ì‚¬ìš© ì˜ˆì œ (ë² ì´ìŠ¤ ì„¤ê³„ í™œìš©)

#### ë‹¨ì¼ ê·¸ë£¹ Optimizer + ìŠ¤ì¼€ì¤„ëŸ¬
```python
model = MyModel()
opt = MyOptimizer(model.parameters(), defaults={"lr": 1e-3})
sched = MyScheduler(opt, last_epoch=-1)

for epoch in range(10):
    for x, y in loader:
        loss = criterion(model(x), y)
        opt.zero_grad()
        loss.backward()
        opt.step()
    sched.step()  # epoch ë‹¨ìœ„ ìŠ¤í…
```

#### ë‹¤ì¤‘ ê·¸ë£¹ + ë‹¤ë¥¸ lr/weight_decay
```python
backbone = {"params": model.backbone.parameters(), "lr": 1e-4}
head = {"params": model.head.parameters(), "lr": 1e-3, "weight_decay": 1e-4}
opt = MyOptimizer([backbone, head], defaults={"lr": 1e-3, "weight_decay": 0.0})
sched = MyScheduler(opt, ...)
```

#### state_dict ì €ì¥/ë¡œë“œ
```python
opt_state = opt.state_dict()
sched_state = sched.state_dict()

# ... save to disk ...

opt.load_state_dict(opt_state)
sched.load_state_dict(sched_state)
```

### ğŸ” ì„¤ê³„ ë…¸íŠ¸ì™€ ì–´ë ¤ì›€

1. **íŒŒë¼ë¯¸í„° íƒ€ì… ê²€ì¦**: optimizerê°€ Tensorë¥¼ ë°›ìœ¼ë©´ grad ì¶”ì ì€ ë˜ì§€ë§Œ ìƒíƒœ ì €ì¥/ì—…ë°ì´íŠ¸ê°€ ì–´ê¸‹ë‚œë‹¤. â†’ ìƒì„± ì‹œ `nn.Parameter` íƒ€ì…ì„ ê°•ì œ ê²€ì¦.  
2. **param_group ì¤‘ë³µ**: ê°™ì€ íŒŒë¼ë¯¸í„°ê°€ ì—¬ëŸ¬ ê·¸ë£¹ì— ë“¤ì–´ê°€ë©´ ì´ì¤‘ ì—…ë°ì´íŠ¸ê°€ ë°œìƒ. â†’ `add_param_group`ì—ì„œ ì¤‘ë³µ ê²€ì‚¬ í›„ ì˜ˆì™¸.  
3. **state_dict ë§¤í•‘**: íŒŒë¼ë¯¸í„° ê°ì²´ ì°¸ì¡°ëŠ” ì§ë ¬í™” ë¶ˆê°€. â†’ ì¸ë±ìŠ¤ë¡œ ë§¤í•‘í•˜ê³  ë¡œë“œì‹œ í˜„ì¬ íŒŒë¼ë¯¸í„° ìˆœì„œë¡œ ì—­ë§¤í•‘.  
4. **ìŠ¤ì¼€ì¤„ëŸ¬-Optimizer ë™ê¸°í™”**: param_group ìˆ˜ê°€ ë‹¤ë¥´ë©´ í•™ìŠµë¥  ë¦¬ìŠ¤íŠ¸ ê¸¸ì´ê°€ ë§ì§€ ì•Šì•„ ëŸ°íƒ€ì„ ì—ëŸ¬. â†’ state_dict ë¡œë“œ ì‹œ ê·¸ë£¹ ìˆ˜ ê²€ì¦.  
5. **step ìˆœì„œ**: ì‚¬ìš©ìê°€ ìŠ¤ì¼€ì¤„ëŸ¬ stepê³¼ optimizer step ìˆœì„œë¥¼ í˜¼ë™í•  ìˆ˜ ìˆìŒ. â†’ PyTorchì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ë¥¼ ìœ ì§€í•˜ê³ , ë¬¸ì„œì—ì„œ ì‚¬ìš© íŒ¨í„´ì„ ì•ˆë‚´.  
6. **verbose/logging**: í•™ìŠµë¥  ë³€ê²½ì„ ì¶”ì í•˜ë ¤ë©´ ë¡œê·¸ê°€ í•„ìš”. â†’ `verbose` í”Œë˜ê·¸ë¡œ ê° ìŠ¤í…ì˜ lrì„ ì¶œë ¥í•˜ëŠ” ì˜µì…˜ ì œê³µ.

---

### âœ… ì •ë¦¬

Lucidì˜ Optimizer/LRScheduler ë² ì´ìŠ¤ëŠ” PyTorch í˜¸í™˜ì„±ì„ ëª©í‘œë¡œ, **Parameterë§Œ ë‹¤ë£¨ëŠ” optimizer**ì™€ **param_group í•™ìŠµë¥ ì„ ê°±ì‹ í•˜ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬**ë¼ëŠ” ë‹¨ìˆœí•œ ì›ë¦¬ë¥¼ ë”°ë¥¸ë‹¤. íŒŒë¼ë¯¸í„°/ë²„í¼ ì‹œìŠ¤í…œ, ëª¨ë“ˆ íŠ¸ë¦¬, state_dict ì§ë ¬í™” íë¦„ê³¼ ê²°í•©í•´, í•™ìŠµ ë£¨í”„ì—ì„œ **opt.zero_grad â†’ backward â†’ opt.step â†’ sched.step** ì´ë¼ëŠ” ìµìˆ™í•œ íŒ¨í„´ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. êµ¬ì²´ì  ì•Œê³ ë¦¬ì¦˜(SGD, Adam, CosineLR ë“±)ì€ ì´ ë² ì´ìŠ¤ ìœ„ì— ì–¹ê¸°ë§Œ í•˜ë©´ ë˜ë©°, ìƒíƒœ ì €ì¥/ë¡œë“œ, ê·¸ë£¹ ê´€ë¦¬, lr ê°±ì‹ ì€ ë² ì´ìŠ¤ê°€ ì¼ê´€ë˜ê²Œ ì²˜ë¦¬í•œë‹¤.
