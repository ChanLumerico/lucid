## âš™ï¸ í…ì„œ Operation ì‹œìŠ¤í…œ êµ¬ì¶•

### ğŸŒ± Tensor í´ë˜ìŠ¤ì˜ êµ¬ì¡°ì  í‹ˆìƒˆ ë§¤ê¾¸ê¸°

Lucidì˜ `Tensor`ê°€ ì–´ëŠ ì •ë„ í˜•íƒœë¥¼ ê°–ì¶”ê¸° ì‹œì‘í–ˆì„ ë•Œ, ë‚˜ëŠ” í•œ ê°€ì§€ **ë¶ˆí¸í•œ ì **ì´ ëˆˆì— ë„ì—ˆë‹¤. Tensor ìì²´ëŠ” ë°ì´í„°ë¥¼ í’ˆê³ , gradientë¥¼ ì¶”ì í•˜ê³ , ê·¸ ì—­ì „íŒŒë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆì„ ë§Œí¼ íŠ¼ì‹¤í•´ì¡Œì§€ë§Œ, ë§‰ìƒ ì—°ì‚° ìì²´ê°€ ì´ë£¨ì–´ì§€ëŠ” ìˆœê°„ì„ ë– ì˜¬ë¦¬ë©´ ë‚´ë¶€ì˜ ëª¨ë“  êµ¬ì„±ë¬¼ì´ _ì¼ê´€ì ì´ê³  ìš°ì•„í•˜ê²Œ ì›€ì§ì´ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì‚¬ì‹¤_ ì´ ë„ˆë¬´ ì„ ëª…í•˜ê²Œ ëŠê»´ì¡Œë‹¤.

`add`, `sub`, `mul`, `matmul` ì²˜ëŸ¼ ê°€ì¥ ê¸°ë³¸ì ì¸ ì—°ì‚°ë“¤ì„ ì‹œë„í•˜ëŠ” ìˆœê°„, Tensor í´ë˜ìŠ¤ê°€ ì•„ë¬´ë¦¬ íƒ„íƒ„í•˜ë”ë¼ë„ ì—°ì‚° ê³¼ì • ìì²´ì—ì„œëŠ” **ë°˜ë³µë˜ëŠ” íŒ¨í„´ê³¼ ì¼ì¢…ì˜ ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸(boilerplate) ì½”ë“œ**ê°€ ë…¸ê³¨ì ìœ¼ë¡œ ë“œëŸ¬ë‚¬ë‹¤.

- ê²°ê³¼ê°’ `Tensor` í•˜ë‚˜ë¥¼ ìƒì„±í•˜ê³ , 
- ë¶€ëª¨ ë…¸ë“œë¥¼ ê¸°ë¡í•˜ê³ , 
- backward ì‹œ ì–´ë–¤ gradientë¥¼ ë¶€ëª¨ë“¤ì—ê²Œ í˜ë ¤ë³´ë‚¼ì§€ ì •ì˜í•˜ê³ , 
- ë§Œì•½ broadcastingì´ ë˜ì–´ìˆìœ¼ë©´ ê·¸ shapeì„ ì¤„ì—¬ ë§ì¶”ê³ , 
- `requires_grad` ì—¬ë¶€ì— ë”°ë¼ backward ì—°ê²° ì—¬ë¶€ë¥¼ ê²°ì •í•˜ê³ ...

ì´ëŸ° ëª¨ë“  ê³¼ì •ì€ `Tensor`ê°€ ì•„ë‹ˆë¼ ë°”ë¡œ _"ì—°ì‚°(operation)"_ ì´ë¼ëŠ” ì¡´ì¬ê°€ ìŠ¤ìŠ¤ë¡œ í•´ì•¼ í•˜ëŠ” ì¼ì²˜ëŸ¼ ë³´ì˜€ì§€ë§Œ, ì •ì‘ ê·¸ ì±…ì„ì„ ì–´ë””ì—ë„ ë‘” ì ì´ ì—†ì—ˆë‹¤.

ì´ë²ˆ ê¸°ë¡ì€ ê·¸ëŸ° ê³µë°±ì„ ì±„ìš°ê¸° ìœ„í•œ Lucidì˜ ì„¸ ë²ˆì§¸ í•µì‹¬ ì¶•ì¸, í…ì„œ ì—°ì‚° ì¶”ìƒí™”ë¥¼ ìœ„í•œ **`operation` í´ë˜ìŠ¤**ì™€ ì´ë¥¼ ê°ì‹¸ëŠ” **`@func_op` ë°ì½”ë ˆì´í„° íŒ©í† ë¦¬**ì˜ í˜•ì„± ê³¼ì •ì„ ë‹´ê³  ìˆë‹¤. ì•ì„œ ì„¤ëª…í•œ ë°°ê²½ ì†ì—ì„œ ì–´ë–»ê²Œ Lucidê°€ ì—°ì‚°ì„ ë°”ë¼ë³´ëŠ” ë°©ì‹ì„ í˜•ì‹í™”í•˜ê³ , autodiff ê·¸ë˜í”„ë¥¼ ì¼ê´€ì ìœ¼ë¡œ êµ¬ì¶•í•˜ëŠ” êµ¬ì¡°ë¥¼ ë§Œë“¤ì—ˆëŠ”ì§€ë¥¼, ê°€ëŠ¥í•œ í•œ ë‚´ì  ê³ ë¯¼ê³¼ í•¨ê»˜ ì„œìˆ í•˜ê³ ì í•œë‹¤.

---

### âš ï¸ ì´ˆì°½ê¸° í…ì„œ ì—°ì‚° êµ¬í˜„ì—ì„œì˜ ë¹„íš¨ìœ¨ì„±

Lucidì˜ ê°€ìƒ ì´ˆì°½ê¸° `add` ì—°ì‚°ì€ ì§€ê¸ˆ ë‹¤ì‹œ ë³´ë©´ ë‹¤ì†Œ **íˆ¬ë°•í•˜ê³  ì›ì‹œì **ì´ë‹¤.

```python
def add(a: Tensor, b: Tensor) -> tuple[Tensor, Callable]:
	ret = Tensor(a.data + b.data)
    ret.requires_grad = a.requires_grad or b.requires_grad
    
    def backward(...) -> tuple[NumPyArray, NumPyArray]:
        # d(ret)/da, d(ret)/db
    	return ret.grad, ret.grad
    
    ret._prev = [a, b]
    ret._op_backward = backward  # í•¨ìˆ˜ ë°”ì¸ë”©
    
    return ret, backward
```

í•˜ì§€ë§Œ ë‹¹ì‹œì—ëŠ” ì¶©ë¶„íˆ í•©ë¦¬ì ì¸ ì¶œë°œì ì´ì—ˆë‹¤. ë¬´ì—‡ë³´ë‹¤ ì¤‘ìš”í•œ ê²ƒì€, `Tensor` ë‚´ë¶€ì— **ìˆ˜ì‘ì—…ìœ¼ë¡œ** ë©”íƒ€ë°ì´í„°ë¥¼ ë¶™ì´ê³  backward ê·œì¹™ì„ ê·¸ ìë¦¬ì—ì„œ ì •ì˜í•´ë²„ë¦¬ëŠ” ë°©ì‹ì´ ì²˜ìŒì—ëŠ” ê½¤ë‚˜ ë‹¨ìˆœí•˜ê²Œ ëŠê»´ì¡Œë‹¤ëŠ” ì ì´ë‹¤.

ê·¸ëŸ¬ë‚˜ ì—°ì‚°ì´ ëŠ˜ì–´ë‚ ìˆ˜ë¡, ì´ ë‹¨ìˆœí•¨ì€ ì ì  **ë¬´ê±°ìš´ ì˜ë¬´ê°**ìœ¼ë¡œ ë³€í•´ê°”ë‹¤.

ì—°ì‚°ì„ í•˜ë‚˜ ë§Œë“¤ ë•Œ ë§ˆë‹¤ ë‚˜ëŠ” ì–¸ì œë‚˜ ê°™ì€ ë¬¸ì¥ì„ ë°˜ë³µí•´ì•¼ í–ˆë‹¤. `requires_grad`ë¥¼ ì „íŒŒí•˜ê³ , ê²°ê³¼ `Tensor`ë¥¼ ë§Œë“¤ê³ , ê·¸ë˜í”„ ì—°ê²°ì„ í•˜ê³ , backwardë¥¼ ë“±ë¡í•˜ê³ ... ì´ ëª¨ë“  ê³¼ì •ì´ ë§ˆì¹˜ **ì¼ì¢…ì˜ ì¼ìƒì ì¸ ì˜ì‹ì²˜ëŸ¼ ë°˜ë³µ**ë˜ì—ˆë‹¤.

`Tensor`ëŠ” ì´ë¯¸ ì¶©ë¶„íˆ ì œ ì—­í• ì„ ë‚˜ë¦„ëŒ€ë¡œ ì˜ í•˜ê³  ìˆëŠ”ë°, ê·¸ í…ì„œë“¤ì´ ì‹¤ì œ ì—°ì‚°ì„ í†µí•´ ë³€í˜•ë  ë•Œì˜ íŒ¨í„´ì€ ì „í˜€ _ì¶”ìƒí™”ë˜ì§€ ì•Šì€ ì±„ë¡œ_ ë°©ì¹˜ë˜ì–´ ìˆì—ˆë‹¤. ê·¸ë˜í”„ëŠ” ë¶„ëª… ì¡´ì¬í•˜ëŠ”ë°, ê·¸ ê·¸ë˜í”„ì˜ ë…¸ë“œë¥¼ ì–´ë–»ê²Œ ë§Œë“¤ì§€ì— ëŒ€í•œ **ì¼ê´€ëœ ê·œì•½**ì´ ì—†ë˜ ê²ƒì´ë‚˜ ë§ˆì°¬ê°€ì§€ì´ë‹¤.

ì´ëŸ¬í•œ ìƒíƒœê°€ ì§€ì†ë˜ë©´ ì½”ë“œë¥¼ ìœ ì§€ ë³´ìˆ˜í•˜ëŠ” ê³¼ì •ì—ì„œ ì‹¤ìˆ˜ë‚˜ ëˆ„ë½ì´ ìƒê¸°ê¸° ë§ˆë ¨ì´ê³ , ì´ëŠ” ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë¡œì„œì˜ Lucidì˜ í•µì‹¬ì„ ë’¤í”ë“œëŠ” **ì ì¬ì ì¸ ì·¨ì•½ì **ì´ ë˜ì—ˆë‹¤.

ì´ ë°˜ë³µì„ ëŠê¸° ìœ„í•´ì„  ë°˜ë“œì‹œ **ì—°ì‚°ì„ ì—°ì‚°ë‹µê²Œ ì²´ê³„í™”í•˜ëŠ” ì¥ì¹˜**ê°€ í•„ìš”í–ˆë‹¤.

### ğŸ”® Operation ê°ì²´ë¼ëŠ” ì‚¬ê³ ì˜ ì „í™˜

ë¬¸ì œì˜ ê·¼ë³¸ì„ íŒŒí—¤ì¹˜ê¸° ìœ„í•´, ë‚˜ëŠ” Tensor ì¤‘ì‹¬ ì‚¬ê³ ì—ì„œ ë²—ì–´ë‚˜ **ì—°ì‚° ìì²´ë¥¼ ê°ì²´ë¡œ ë§Œë“¤ìëŠ” ê²°ë¡ **ì— ë„ë‹¬í–ˆë‹¤. ì •í™•í•˜ê²Œ ë§í•˜ìë©´, `operation`ì´ë¼ëŠ” _ë¶€ëª¨ í´ë˜ìŠ¤_ ë¥¼ ì²´ê³„í™” í•œ ë’¤, ê° ì—°ì‚°ë“¤(`add`, `sub` ë“±)ì„ ì´ `operation`ì„ ìƒì†í•˜ëŠ” _ìì‹ í´ë˜ìŠ¤_ ë¡œì¨ êµ¬í˜„í•˜ì—¬ **í…ì„œ ì—°ì‚°ì„ ì¶”ìƒí™”**í•˜ëŠ” ì•„ì´ë””ì–´ë¥¼ ë– ì˜¬ë ¸ë‹¤.

ì—°ì‚°(operation)ì„ í•˜ë‚˜ì˜ ì‘ì€ ì‘ì—… ë‹¨ìœ„ë¼ê³  ìƒê°í•´ë³´ë©´ ê·¸ ì‘ì—…ì´ í•˜ëŠ” ì¼ì€ í¬ê²Œ ë‘ ê°€ì§€ ë¿ì´ë‹¤.

**1.** Forward ê³„ì‚°ì„ ìˆ˜í–‰í•œë‹¤.
**2.** Backward ê³„ì‚° ì‹œ í•„ìš”í•œ ì •ë³´ë¥¼ ì €ì¥í•˜ê³ , gradientë¥¼ ì—­ìœ¼ë¡œ ê³„ì‚°í•œë‹¤.

ì´ ë‘ ì¶•ë§Œ ëª…í™•íˆ ë¶„ë¦¬ëœë‹¤ë©´, ë‚˜ë¨¸ì§€ Tensorì— ê´€ë ¨ëœ ë©”íƒ€ë°ì´í„° ì‘ì—…ì€ ë³„ë„ì˜ ë£¨í‹´ì„ ë§Œë“¤ë©´ ë  ê²ƒì´ë¼ê³  ìƒê°í–ˆë‹¤. ê·¸ë˜ì„œ ì²˜ìŒì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìµœì†Œí•œì˜ `operation` í´ë˜ìŠ¤ë¥¼ ìŠ¤ì¼€ì¹˜í–ˆë‹¤.

```python
from abc import ABC, abstractmethod

class operation(ABC):
	def __init__(self, *args, **kwargs) -> None: ...
    
    @abstractmethod
    def forward(self, *args) -> Tensor | tuple[Tensor, ...]: ...
    
    @abstractmethod
    def backward(self, grad_out) -> Tensor | tuple[Tensor, ...]: ...
    
    def __call__(self, *args) -> Tensor | tuple[Tensor, ...]:
    	return self.forward(*args)
```

ì´ í”„ë¡œí† íƒ€ì…ì€ ë‚˜ë¦„ í›Œë¥­í–ˆë‹¤. ì—°ì‚°ì„ forward/backward ë‘ ê´€ì ì—ì„œ êµ¬ì¡°í™”í•  ìˆ˜ ìˆì—ˆê³ , ê° ì—°ì‚°ë§ˆë‹¤ í•„ìš”ë¡œ í•˜ëŠ” ë³„ë„ì˜ ì†ì„±(attribute)ë“¤ì„ ë”°ë¡œ ì €ì¥í•  ìˆ˜ë„ ìˆì—ˆë‹¤. í•˜ì§€ë§Œ ê³§ **ë˜ ë‹¤ë¥¸ ì¤‘ë³µ**ì´ ë“œëŸ¬ë‚¬ë‹¤. ì—¬ì „íˆ ê²°ê³¼ Tensorë¥¼ ë§Œë“œëŠ” ê³¼ì •, `_prev`, `_op`ë¥¼ ì„¸íŒ…í•˜ëŠ” ê³¼ì •, backward í´ë¡œì ¸(closure)ë¥¼ ì •ì˜í•˜ëŠ” ê³¼ì • ë“±ì˜ ì¼ë ¨ì˜ ê³µí†µëœ ë¡œì§ì„ **ëª¨ë“  ì—°ì‚° í´ë˜ìŠ¤ êµ¬í˜„ìê°€ ì§ì ‘ ì‘ì„±**í•´ì•¼ í•œë‹¤ëŠ” ë¬¸ì œê°€ ë‚¨ì•„ ìˆì—ˆë‹¤.

ì¦‰, `operation` í´ë˜ìŠ¤ë¼ëŠ” í˜•ì‹ì€ _forward/backwardë¥¼ ë‹´ëŠ” ê·¸ë¦‡_ ìœ¼ë¡œì„œ í›Œë¥­í–ˆì§€ë§Œ, **Tensor ê·¸ë˜í”„ ì—°ê²°**ì´ë¼ëŠ” ë³¸ì§ˆì  ë°˜ë³µ ë¬¸ì œëŠ” ì—¬ì „íˆ í•´ê²°ë˜ì§€ ì•Šì•˜ë‹¤.

`operation`ì€ ìµœì‹  ë²„ì „ì˜ Lucidì—ì„œëŠ” ë§ì´ ë‹¤ë¥¸ í˜•íƒœë¥¼ ë„ê³  ìˆìœ¼ë©°, [`lucid._backend.core.operation`](https://github.com/ChanLumerico/lucid/blob/main/lucid/_backend/core.py#L130)ì— ì¡´ì¬í•˜ë©° `.core` namespaceëŠ” ìƒëµ ê°€ëŠ¥í•˜ë‹¤.

### ğŸ”¬ `@func_op` ë°ì½”ë ˆì´í„° íŒ©í† ë¦¬ì˜ ë“±ì¥

ê²°êµ­ ë‚˜ëŠ” _"ì—°ì‚°ì´ í•´ì•¼ í•˜ëŠ” ê²ƒì€ forward/backward ë¿ì´ë‹¤"_ ë¼ëŠ” ëª…ì œë¥¼ ë‹¤ì‹œ ë– ì˜¬ë ¸ë‹¤. ê·¸ ì™¸ì˜ ëª¨ë“  ì‘ì—…, ì¦‰ ìƒˆ `Tensor`ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„° ì—°ê²°, backward closure êµ¬ì„±, shape ì •ë ¬ ê°™ì€ ìˆ˜ì‘ì—…ë“¤ì€ **ì—°ì‚°ìì—ê²Œ ë– ë„˜ê¸¸ í•„ìš”ê°€ ì „í˜€ ì—†ì—ˆë‹¤**.

ê·¸ ìˆœê°„ ë¬¸ë“ **"ë°ì½”ë ˆì´í„°(decorator)"** ë¼ëŠ” ê°œë…ì´ ë– ì˜¬ëë‹¤. 

>ì—°ì‚°ìëŠ” ì˜¤ì§ ì‹¤ì œë¡œ ê³„ì‚°ë˜ëŠ” forward í•¨ìˆ˜ì™€ ê·¸ì— ëŒ€ì‘í•˜ëŠ” gradient ê³„ì‚° í•¨ìˆ˜ë§Œ ì •ì˜í•˜ê³ , ì•ì„œ ì–¸ê¸‰í•œ ë‚˜ë¨¸ì§€ ëª¨ë“  Tensor ê´€ë ¨ ì‘ì—…ë“¤ì€ _ë°ì½”ë ˆì´í„°ê°€ ìë™ìœ¼ë¡œ ìˆ˜í–‰_ í•œë‹¤ë©´ ì–´ë–¨ê¹Œ?

ì´ ë°œìƒì€ ê³§ë°”ë¡œ `@func_op`ë¼ëŠ” **ë°ì½”ë ˆì´í„° íŒ©í† ë¦¬(decorator factory)** ì˜ íƒ„ìƒìœ¼ë¡œ ì´ì–´ì¡Œë‹¤. ë°ì½”ë ˆì´í„°ëŠ” ì—°ì‚° í•¨ìˆ˜ì˜ ì±…ì„ì„ ì¢íˆê³ , ê·¸ì™¸ì˜ ìš”ì†ŒëŠ” ëª¨ë‘ ê³µí†µ í…œí”Œë¦¿ìœ¼ë¡œ ëŒì–´ì•ˆëŠ”ë‹¤. ë§ˆì¹˜ ì—°ì‚° êµ¬í˜„ìê°€ _ìˆ˜í•™ì  ì •ì˜ë§Œ ì¨ë†“ìœ¼ë©´_ Lucidê°€ ê·¸ë˜í”„ ì—°ê²°ì„ ëŒ€ì‹  ë‹¤ ì²˜ë¦¬í•´ì£¼ëŠ” í˜•íƒœê°€ ë˜ëŠ” ê²ƒì´ë‹¤.

ì´ë•Œ ë‚˜ëŠ” ì—°ì‚° ì •ì˜ìê°€ **ë°˜ë“œì‹œ** ë‹¤ìŒ í˜•ì‹ì„ return í•˜ë„ë¡ ê·œì•½ì„ ì„¸ì› ë‹¤.

```python
((result_tensor, grad_function), ...)
```

ì—¬ê¸°ì„œ `grad_function`ì€ backward ì‹œ í˜¸ì¶œë˜ë©°, **ì…ë ¥ í…ì„œ ê°œìˆ˜ì— ë§ì¶˜ gradient íŠœí”Œ**ì„ ëŒë ¤ì¤€ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì–´ë–¤ operation $\ast$ê°€ $x,y$ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ $z$ë¥¼ return í•œë‹¤ë©´ ($x\ast y=z$), ì´ operationì€ í•˜ë‚˜ì˜ `(result_tensor, grad_function)` íŠœí”Œì„ returní•˜ê³ , ì´ `grad_function`ì€ $\partial z/\partial x$ì™€ $\partial z/\partial y$ íŠœí”Œì„ returní•œë‹¤. ì´ í˜•íƒœë§Œ í†µì¼í•´ë‘ë©´ ë‚˜ë¨¸ì§€ëŠ” ë°ì½”ë ˆì´í„°ê°€ ì–´ë–»ê²ŒëŠ” ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤.

`@func_op`ì€ [`lucid._backend.core.func_op`](https://github.com/ChanLumerico/lucid/blob/main/lucid/_backend/core.py#L20)ì— ì¡´ì¬í•˜ë©°, `.core` namespaceëŠ” ìƒëµí•´ë„ ë§ˆì°¬ê°€ì§€ë¡œ ì ‘ê·¼ì´ ê°€ëŠ¥í•˜ë‹¤.

### ğŸ§¬ `@func_op` ë‚´ë¶€ ì•Œê³ ë¦¬ì¦˜ íŒŒí—¤ì¹˜ê¸°

ì´ì œ ì‹¤ì œë¡œ `@func_op`ì´ ì‹¤ì œë¡œ ì–´ë–»ê²Œ ìƒê²¼ëŠ”ì§€ë¥¼ í° ë©ì–´ë¦¬ í•˜ë‚˜ë¡œ ë³´ëŠ” ëŒ€ì‹ , ì‘ì€ **code snippetë“¤ë¡œ ì˜ê²Œ ë¶„í•´í•´ì„œ** ì‚´í´ë³´ë ¤ í•œë‹¤. ê° snippetì€ ë‹¹ì‹œ ë‚´ê°€ ì–´ë–¤ ê³ ë¯¼ì„ í–ˆëŠ”ì§€, ê·¸ë¦¬ê³  ì´ ì¶”ìƒí™”ê°€ ì–´ë–¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ë ¤ê³  í–ˆëŠ”ì§€ë¥¼ ê·¸ëŒ€ë¡œ ë°˜ì˜í•˜ê³  ìˆë‹¤.

**ê°€ì¥ ë°”ê¹¥ìª½ ê³¨ê²©**ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

```python
import functools

def func_op(n_in: int, n_ret: int | None, has_gradient: bool = True) -> Callable:
	def decorator(func: Callable) -> Callable:
    	@functools.wraps(func)
        def wrapper(op_self: operation, *args, **kwargs) -> Tensor | tuple[Tensor, ...]:
        	...
        return wrapper
    return decorator
```

ê²‰ìœ¼ë¡œ ë³´ë©´ ë‹¨ìˆœí•œ decorator factoryì´ë‹¤. í•˜ì§€ë§Œ ì´ ì•ˆìª½ `wrapper`ê°€ Lucidì˜ ëª¨ë“  `operation` í˜¸ì¶œì„ ê´€í†µí•˜ëŠ” **ê³µí†µ í…œí”Œë¦¿**ì´ ëœë‹¤.

- `n_in`ì€ ì—°ì‚°ì˜ ì¸ìë“¤ ì¤‘ ì•ì—ì„œë¶€í„° ëª‡ ê°œë¥¼ `Tensor`ë¡œ ì·¨ê¸‰í•  ê²ƒì¸ì§€ë¥¼,
- `n_ret`ì€ ì´ ì—°ì‚°ì´ ëª‡ ê°œì˜ `Tensor`ë¥¼ returní•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
- `has_gradient`ëŠ” ë§ ê·¸ëŒ€ë¡œ ì´ ì—°ì‚°ì´ _ë¯¸ë¶„ ê°€ëŠ¥í•œ ì—°ì‚°ì¸ì§€_ ì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í”Œë˜ê·¸ì´ë‹¤.

ì´ ì„¸ ê°œì˜ ê°’ë§Œìœ¼ë¡œ `wrapper`ëŠ” ì—°ì‚°ì˜ _í˜•íƒœ_ ë¥¼ ì´í•´í•˜ê³ , ë‚˜ë¨¸ì§€ ì„¸ë¶€ êµ¬í˜„ì€ `func`ë¼ëŠ” **callbackì— ìœ„ì„**í•˜ëŠ” ë°©ì‹ì´ë‹¤.

#### 1ï¸âƒ£ `wrapper` ë‚´ë¶€ ìƒíƒœ ì´ˆê¸°í™”

`wrapper`ê°€ í˜¸ì¶œë˜ë©´ ê°€ì¥ ë¨¼ì € Tensor ì¸ìë“¤ì„ ëª¨ì•„ë‘˜ ì»¨í…Œì´ë„ˆì™€ `requires_grad` í”Œë˜ê·¸ë¥¼ **ì´ˆê¸°í™”**í•œë‹¤.

```python
def wrapper(op_self: operation, *args, **kwargs) -> Tensor | tuple[Tensor, ...]:
	tensors: tuple[Tensor] = ()
    requires_grad = False
    ...
```

ì—¬ê¸°ì„œ `tensors`ëŠ” ë‚˜ì¤‘ì— backward ë•Œ gradientë¥¼ í˜ë ¤ë³´ë‚´ì•¼ í•  **ë¶€ëª¨ Tensorë“¤ì„ ìˆœì„œëŒ€ë¡œ ë‹´ëŠ” íŠœí”Œ**ì´ë‹¤. ì´ ìˆœì„œê°€ ì¤‘ìš”í•œ ì´ìœ ëŠ”, grad functionì´ ë°˜í™˜í•˜ëŠ” gradient íŠœí”Œ ì—­ì‹œ ì…ë ¥ Tensorì™€ _ë™ì¼í•œ ìˆœì„œë¥¼_ ë”°ë¼ì•¼ í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

`requires_grad`ëŠ” ëª¨ë“  ì…ë ¥ Tensorë¥¼ í›‘ì€ ë’¤ _"ì´ ì—°ì‚° ê²°ê³¼ê°€ gradientë¥¼ ê³„ì‚°í•´ì•¼ í•˜ëŠ”ì§€"_ ë¥¼ ê²°ì •í•˜ëŠ” **ì „ì—­ì ì¸ í”Œë˜ê·¸**ê°€ ëœë‹¤. ì´ ë‘ ë³€ìˆ˜ëŠ” `wrapper`ì˜ ë‚˜ë¨¸ì§€ ë¡œì§ì´ ëª¨ë‘ ê³µìœ í•˜ëŠ” ì‘ì€ stateë“¤ì´ë‹¤.

ì´ˆê¸° êµ¬í˜„ì—ì„œëŠ” ì´ ë‘ ë³€ìˆ˜ë¥¼ ë”°ë¡œ ë‘ì§€ ì•Šê³ , ì¸ì ì²˜ë¦¬ì™€ gradient ì „íŒŒë¥¼ ì„ì–´ì„œ ì²˜ë¦¬í•˜ë ¤ê³  í–ˆì—ˆë‹¤. í•˜ì§€ë§Œ ê³§ ì½ê¸°ë„ ì–´ë µê³ , ë””ë²„ê¹…ì„ í•˜ê¸°ë„ í˜ë“¤ë‹¤ëŠ” ì‚¬ì‹¤ì„ ê¹¨ë‹«ê³  ì´ì²˜ëŸ¼ `wrapper`ì˜ ì‹œì‘ ë¶€ë¶„ì—ì„œ **ê³µí†µ state**ë¥¼ ë¶„ë¦¬í•´ ì„ ì–¸í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì •ë¦¬í–ˆë‹¤.

#### 2ï¸âƒ£ Tensor ì¸ì êµ¬ê°„ ì˜ë¼ë‚´ê¸°

ë‹¤ìŒ snippetì€ **ì–´ë””ê¹Œì§€ë¥¼ Tensor ì¸ìë¡œ ë³¼ ê²ƒì¸ê°€**ë¥¼ ê²°ì •í•˜ëŠ” ë¶€ë¶„ì´ë‹¤.

```python
def wrapper(op_self: operation, *args, **kwargs) -> Tensor | tuple[Tensor, ...]:
	...
    if n_in is None:
    	tensor_args = args
    else:
    	if len(args) < n_in:
        	raise ValueError(
            	f"Expected at least {n_in} tensor arguments, got {len(args)}"
            )
        tensor_args = args[:n_in]
	...
```

`n_in`ì´ `None`ì´ë©´ ì´ ì—°ì‚°ì€ _"ì•ì— ì˜¤ëŠ” ëª¨ë“  ì¸ì"_ ë¥¼ `Tensor`ë¡œ ì·¨ê¸‰í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ `sum(*xs, axis=None)`ê°™ì€ **polyadic ì—°ì‚°**ì—ì„œ ìœ ìš©í•˜ë‹¤. ë°˜ëŒ€ë¡œ `n_in`ì´ êµ¬ì²´ì ì¸ ì •ìˆ˜ë¼ë©´, ê·¸ë§Œí¼ë§Œ ì•ì—ì„œ ì˜ë¼ `tensor_args`ë¡œ ê°„ì£¼í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” `axis`, `keepdims`, `shape` ì²˜ëŸ¼ ì—°ì‚°ì˜ ëª¨ì–‘ì„ ì œì–´í•˜ëŠ” **ë¶€ê°€ì ì¸ í‚¤ì›Œë“œ ì¸ì**ë¡œ ë³¸ë‹¤.

ì¤‘ìš”í•œ ì ì€, ì´ ì‹œì ì—ì„œ ì•„ì§ `tensor_args`ì˜ ì›ì†Œë“¤ì´ ì‹¤ì œë¡œ `Tensor`ì¸ì§€ ì•„ë‹Œì§€ëŠ” **í™•ì¸í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒ**ì´ë‹¤. ì´ ë‹¨ê³„ì˜ ëª©ì ì€ ì˜¤ì§ _"ìœ„ì¹˜ ê¸°ë°˜ìœ¼ë¡œ Tensor í›„ë³´ë“¤ì„ ë¶„ë¦¬í•´ ë‚´ëŠ” ê²ƒ"_ ì´ë‹¤.

íƒ€ì… ê°•ì œ castingì€ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ **ì¼ê´„ì ìœ¼ë¡œ ìˆ˜í–‰**ëœë‹¤. ì´ë ‡ê²Œ êµ¬ê°„ ë¶„ë¦¬ì™€ íƒ€ì… ì •ì œë¥¼ ë‚˜ëˆ ë‘” ì´ìœ ëŠ”, ë‚˜ì¤‘ì— í•¨ìˆ˜ signatureì„ ë°”ê¾¸ê±°ë‚˜ ì¸ì ìˆ˜ë¥¼ ì¡°ì •í•  ë•Œ ì–´ëŠ ë¶€ë¶„ì„ ìˆ˜ì •í•´ì•¼ í• ì§€ê°€ ëª…í™•í•´ì§€ê¸° ë•Œë¬¸ì´ë‹¤.

#### 3ï¸âƒ£ Tensor ê°•ì œ ìºìŠ¤íŒ…ê³¼ `requires_grad` ì „íŒŒ

ì´ì œ Tensor í›„ë³´ë“¤ì„ ì‹¤ì œ `Tensor`ë¡œ **ì •ì œ**í•˜ëŠ” ê³¼ì •ì´ ì´ì–´ì§„ë‹¤.

```python
def wrapper(op_self: operation, *args, **kwargs) -> Tensor | tuple[Tensor, ...]:
	...
    for arg in tensor_args:
    	tensor = lucid._check_is_tensor(arg)
        tensors += (tensor,)
        requires_grad = requires_grad or tensor.requires_grad
    ...
```

ì—¬ê¸°ì„œ `lucid._check_is_tensor`ëŠ” ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì „ì²´ì—ì„œ ì“°ì´ëŠ” ì‘ì€ ìœ í‹¸ë¦¬í‹°ë¡œ, `Tensor`ê°€ ì•„ë‹Œ ì…ë ¥(ì˜ˆ: `int | float | NumPyArray` ë“±)ì„ ì ì ˆíˆ `Tensor`ë¡œ **ìŠ¹ê²©**ì‹œí‚¤ëŠ” ì—­í• ì„ í•˜ë©° ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë˜ì–´ìˆë‹¤.

```python
def _check_is_tensor(any: Tensor | _ArrayOrScalar) -> Tensor:
    if not isinstance(any, Tensor):
        return Tensor(any)
    return any
```

ì´ í•¨ìˆ˜ë¥¼ `wrapper` ë‚´ë¶€ì—ì„œ í˜¸ì¶œí•¨ìœ¼ë¡œì¨ operation êµ¬í˜„ìëŠ” **ë” ì´ìƒ ì´ ì¸ìê°€ Tensorì¸ì§€ ì•„ë‹Œì§€ë¥¼ ê±±ì •í•  í•„ìš”ê°€ ì—†ì–´ì§„ë‹¤**. í•­ìƒ `Tensor`ë¼ê³  ê°€ì •í•˜ê³  operation ì½”ë“œë¥¼ ì‘ì„±í•˜ë©´ ë˜ëŠ” ê²ƒì´ë‹¤.

ë°˜ë©´ `requires_grad`ëŠ” ëª¨ë“  Tensorë¥¼ ìˆœíšŒí•˜ë©´ì„œ **í•œ ë²ˆì´ë¼ë„** `True`ê°€ ë°œê²¬ë˜ë©´ ëê¹Œì§€ `True`ë¡œ ìœ ì§€ëœë‹¤. ì´ ê°’ì€ ë‚˜ì¤‘ì— ê²°ê³¼ Tensorì˜ `requires_grad`ë¥¼ ì„¤ì •í•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤. ì¦‰, ë¶€ëª¨ ì¤‘ ë‹¨ í•˜ë‚˜ë¼ê³  gradientë¥¼ ì¶”ì í•˜ê³  ìˆë‹¤ë©´, ì´ ì—°ì‚°ì˜ ê²°ê³¼ ì—­ì‹œ **ë°˜ë“œì‹œ computation graphì— ì—°ê²°ë˜ì–´ì•¼ í•œë‹¤**ëŠ” ëœ»ì´ë‹¤. ì´ ê·œì¹™ì€ PyTorchë¥¼ í¬í•¨í•œ ëŒ€ë¶€ë¶„ì˜ autodiff ì‹œìŠ¤í…œë“¤ì´ ì±„íƒí•˜ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ì „íŒŒ ê·œì¹™ì´ë‹¤.

ë‚´ê°€ ì´ ì½”ë“œë¥¼ ì²˜ìŒ ì§°ì„ ë•Œ, `requires_grad`ë¥¼ ë§¤ë²ˆ `AND`/`OR` ì¤‘ ë¬´ì—‡ìœ¼ë¡œ ì¡°í•©í• ì§€ê°€ ì ê¹ í—·ê°ˆë ¸ë˜ ì ì´ ìˆë‹¤. ì§ê´€ì ìœ¼ë¡œ ìƒê°í•˜ë©´ _"ëª¨ë“  ë¶€ëª¨ê°€ `True`ì¼ ë•Œë§Œ ê²°ê³¼ë„ `True`"_ ë¼ê³  ì˜¤í•´í•˜ê¸° ì‰½ì§€ë§Œ, ì‹¤ì œë¡œëŠ” **í•œ ë¶€ëª¨ë¼ë„** ì¶”ì  ëŒ€ìƒì´ë©´ ê²°ê³¼ë„ gradient ê²½ë¡œ ìœ„ì— ìˆì–´ì•¼ í•œë‹¤. ê·¸ë˜ì„œ `OR`ì„ ì„ íƒí•˜ê²Œ ë˜ì—ˆê³ , ì´ë¥¼ ì •ë¦¬í•˜ë©´ì„œ ì´í›„ ì—°ì‚°ë“¤ì´ ì–´ë–»ê²Œ ì—°ê²°ë˜ë”ë¼ë„ gradient ê²½ë¡œê°€ ëŠê¸°ì§€ ì•ŠëŠ”ë‹¤ëŠ” í™•ì‹ ì„ ì–»ê²Œ ë˜ì—ˆë‹¤.

#### 4ï¸âƒ£ Tensor ì¸ìì™€ ë¹„-Tensor ì¸ì ì¬ì¡°í•©

Tensor í›„ë³´ë“¤ì„ ëª¨ë‘ ì •ì œí–ˆë‹¤ë©´, ì´ì œ ë‚˜ë¨¸ì§€ ì¸ìë“¤ê³¼ ë‹¤ì‹œ í•©ì³ì„œ ì—°ì‚° êµ¬í˜„ìì—ê²Œ ë„˜ê²¨ì¤„ ì¤€ë¹„ë¥¼ í•œë‹¤.

```python
def wrapper(op_self: operation, *args, **kwargs) -> Tensor | tuple[Tensor, ...]:
	...
    non_tensor_args = args[n_in:] if n_in is not None else ()
    new_args = (*tensors, *non_tensor_args)
    ...
```

ì—¬ê¸°ì„œ í•µì‹¬ì€, ì—°ì‚° êµ¬í˜„ìê°€ ë°›ëŠ” ì¸ì ì‹œí€€ìŠ¤ `new_args`ê°€ ë‹¤ìŒ **ë‘ ê°€ì§€ ì„±ì§ˆ**ì„ ë§Œì¡±í•œë‹¤ëŠ” ê²ƒì´ë‹¤.

**1.** ì•ìª½ì—ëŠ” í•­ìƒ ì •ì œëœ `Tensor`ë“¤ë§Œ ì˜¨ë‹¤.
**2.** ê·¸ ë’¤ì— `axis`, `keepdims`, `shape` ë“± ê° ì—°ì‚° íŠ¹ìœ ì˜ _ë©”íƒ€ ì¸ì_ ë“¤ì´ ê·¸ëŒ€ë¡œ ì´ì–´ì§„ë‹¤.

ì´ë ‡ê²Œ í•´ë‘ë©´ ì˜ˆë¥¼ ë“¤ì–´ `add(self, x, y, axis=None)` ê°™ì€ í•¨ìˆ˜ë¥¼ êµ¬í˜„í•  ë•Œ, `x`ì™€ `y`ëŠ” ì´ë¯¸ `Tensor`ë¼ëŠ” ê²ƒì„ ê°€ì •í•  ìˆ˜ ìˆê³ , `axis`ëŠ” ì¶”ê°€ ì²˜ë¦¬ ì—†ì´ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

ì´ ì‘ì€ ë ˆì´ì–´ê°€ ìƒê¹€ìœ¼ë¡œì¨, ì—°ì‚° êµ¬í˜„ë¶€ì—ì„œëŠ” íƒ€ì… ì²´í¬ë‚˜ `Tensor` wrapping ê°™ì€ ì¡ì¼(?)ì„ **ì „í˜€ ì‹ ê²½ ì“¸ í•„ìš”ê°€ ì—†ê²Œ ëœë‹¤**.

#### 5ï¸âƒ£ ì—°ì‚° êµ¬í˜„ì í˜¸ì¶œê³¼ ë¦¬í„´ê°’ ê·œì•½

ì¤€ë¹„ëœ ì¸ìë“¤ì„ ê°€ì§€ê³  ë§ˆì¹¨ë‚´ ì—°ì‚° êµ¬í˜„ì `func`ë¥¼ í˜¸ì¶œí•œë‹¤.

```python
def wrapper(op_self: operation, *args, **kwargs) -> Tensor | tuple[Tensor, ...]:
	...
    func_return_pairs = func(op_self, *new_args, **kwargs)
    ...
```

ì—¬ê¸°ì„œ `func`ëŠ” ì‚¬ìš©ìê°€ `@func_op(...)`ë¡œ ê°ì‹¸ëŠ” ì‹¤ì œ ì—°ì‚° ë©”ì„œë“œ, ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœì´ë‹¤.

```python
class add(operation):
	def __init__(self) -> None:
    	super().__init__()
    
    @func_op(n_in=2, n_ret=1)
    def forward(self, a: Tensor, b: Tensor) -> _FuncOpReturnType:
    	self.result = Tensor(a.data + b.data)
        # ê²°ê³¼ í…ì„œì™€ backward ë©”ì„œë“œ ìŒ ë°˜í™˜
        return self.result, self.backward
    
    def backward(self) -> _GradFuncType:
    	# d(a+b)/da, d(a+b)/db ë°˜í™˜ (chain rule ì ìš© í›„)
    	return self.result.grad, self.result.grad
```

ì´ëŠ” ì‹¤ì œ ì´ˆì°½ê¸° Lucidì˜ í…ì„œ í•© ì—°ì‚°(`add`)ì˜ êµ¬í˜„ì²´ì´ë‹¤. ìœ„ ì½”ë“œë¥¼ í†µí•´ ì•Œ ìˆ˜ ìˆë“¯ì´, êµ¬í˜„ìëŠ” **ë°˜ë“œì‹œ ì—°ì‚°ì˜ forward ë©”ì„œë“œì—ì„œ** `(Tensor, grad_function)` ìŒì˜ íŠœí”Œ(ë“¤)ì„ returní•´ì•¼ í•œë‹¤.

ì´ ê·œì•½ ë•ë¶„ì— `wrapper`ëŠ” ì—°ì‚° ì¢…ë¥˜ì— ìƒê´€ì—†ì´ **í•­ìƒ ë™ì¼í•œ íŒ¨í„´**ìœ¼ë¡œ ê²°ê³¼ë¥¼ í’€ì–´ë‚¼ ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ `grad_function`ì€ backward ì‹œ í˜¸ì¶œë˜ë©°, **ì…ë ¥ Tensor ê°œìˆ˜ë§Œí¼**ì˜ NumPy array gradientë¥¼ returní•˜ëŠ” ìˆœìˆ˜ í•¨ìˆ˜ì´ë‹¤.

#### 6ï¸âƒ£ ê²°ê³¼ ê°œìˆ˜ ì •ë¦¬ì™€ ë‹¨ì¼ ë¦¬í„´ê°’ ì²˜ë¦¬

ì—°ì‚°ë§ˆë‹¤ ë¦¬í„´í•˜ëŠ” Tensorì˜ ê°œìˆ˜ëŠ” **ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤**. ì–´ë–¤ ì—°ì‚°ì€ ìŠ¤ì¹¼ë¼ í•˜ë‚˜ë§Œ, ì–´ë–¤ ì—°ì‚°ì€ ì„¸ ê°œì˜ í…ì„œë¥¼ ë™ì‹œì— ë°˜í™˜í•  ìˆ˜ë„ ìˆë‹¤. ì´ë¥¼ ì¼ë°˜í™”í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ë¡œì§ì„ ì‚¬ìš©í–ˆë‹¤.

```python
def wrapper(op_self: operation, *args, **kwargs) -> Tensor | tuple[Tensor, ...]:
	...
	if n_ret is None:
    	if not isinstance(func_return_pairs, tuple):
        	# ì—°ì‚°ì´ (ê²°ê³¼ í…ì„œ, ë¯¸ë¶„ ë©”ì„œë“œ) íŠœí”Œì„ ë°˜í™˜í•˜ì§€ ì•Šì„ ë•Œì˜ ì˜ˆì™¸ ì²˜ë¦¬
         	raise ValueError(...)
        num_returns = len(func_return_pairs)
    else:
    	num_returns = n_ret
    
    if num_returns == 1:
    	func_return_pairs = (func_return_pairs,)
    ...
```

`n_ret`ê°€ ëª…ì‹œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì‹¤ì œ ë¦¬í„´ê°’ ê¸¸ì´ì—ì„œ **ê°œìˆ˜ë¥¼ ì¶”ë¡ **í•œë‹¤. ì´ë•Œ ë¦¬í„´ê°’ì´ `tuple`ì´ ì•„ë‹ˆë©´ _ê·œì•½ ìœ„ë°˜_ ìœ¼ë¡œ ë³´ê³  ë°”ë¡œ ì—ëŸ¬ë¥¼ ë°œìƒì‹œí‚¨ë‹¤. ê·¸ë¦¬ê³  ë‹¨ì¼ ë¦¬í„´ê°’ì¸ ê²½ìš°ì—ëŠ” `(out, grad)` í˜•íƒœë¡œ ë¦¬í„´ë˜ë”ë¼ê³  ë‚´ë¶€ì ìœ¼ë¡œëŠ” í†µì¼ì„±ì„ ìœ„í•´ `((out, grad),)`ë¼ëŠ” **1-tupleë¡œ ì •ê·œí™”**í•œë‹¤.

ì´ë ‡ê²Œ í•´ë‘ë©´ ì´í›„ ë¡œì§ì—ì„œ _ë‹¨ì¼ ë¦¬í„´_ ê³¼ _ë‹¤ì¤‘ ë¦¬í„´_ ì„ êµ¬ë¶„í•  í•„ìš” ì—†ì´ **ë™ì¼í•œ ë£¨í”„ êµ¬ì¡°**ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤.

#### 7ï¸âƒ£ ê²°ê³¼ Tensorì˜ ë©”íƒ€ë°ì´í„° ì„¤ì •

ì´ì œ `wrapper`ëŠ” ì‹¤ì œ ê²°ê³¼ Tensorë“¤ì„ í•˜ë‚˜ì”© ìˆœíšŒí•˜ë©´ì„œ, ê° ê²°ê³¼ì— ëŒ€í•´ **ê·¸ë˜í”„ ë©”íƒ€ë°ì´í„°**ë¥¼ ì±„ì›Œ ë„£ëŠ”ë‹¤.

```python
def wrapper(op_self: operation, *args, **kwargs) -> Tensor | tuple[Tensor, ...]:
	...
    results: tuple[Tensor] = ()
    for result, compute_grad in func_return_pairs:
    	result.requires_grad = requires_grad and has_gradient
        result._op = op_self  # operation ì¸ìŠ¤í„´ìŠ¤
        results += (result,)
    	...
```

ì—¬ê¸°ì„œ `requires_grad`ëŠ” ì•ì„œ ì…ë ¥ Tensorë“¤ì—ì„œ ê³„ì‚°í•œ ì „ì—­ í”Œë˜ê·¸ì™€, ì—°ì‚° ìì²´ê°€ ë¯¸ë¶„ ê°€ëŠ¥í•œì§€ì— ëŒ€í•œ `has_gradient`ë¥¼ **í•¨ê»˜ ê³ ë ¤**í•œë‹¤. ì´ ë‘˜ì„ `AND` í•œ ê°’ì´ ìµœì¢…ì ìœ¼ë¡œ ê²°ê³¼ Tensorì— ì €ì¥ëœë‹¤.

`_op`ëŠ” ì´ Tensorë¥¼ ìƒì„±í•œ ì—°ì‚° ê°ì²´, ì¦‰ graph ìƒì—ì„œ ì´ Tensorì˜ **ìƒì„± ì›ì¸**ì„ ê°€ë¦¬í‚¨ë‹¤. ë‚˜ì¤‘ì— backwardë¥¼ ìˆ˜í–‰í•  ë•Œ, TensorëŠ” ìì‹ ì„ ë§Œë“  operationì˜ `backward` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•´ ìì‹ ì„ ë§Œë“  **ë¶€ëª¨ë“¤ì—ê²Œ gradientë¥¼ ì „íŒŒ**í•˜ê²Œ ëœë‹¤.

ì´ë ‡ê²Œ ê²°ê³¼ë¥¼ `results` íŠœí”Œì— ìŒ“ì•„ë‘ëŠ” ì´ìœ ëŠ”, ì—¬ëŸ¬ ê°œì˜ Tensorë¥¼ ë¦¬í„´í•˜ëŠ” ì—°ì‚°ì—ì„œë„ **ì¼ê´€ëœ ë¦¬í„´ í˜•íƒœë¥¼ ìœ ì§€**í•˜ê¸° ìœ„í•¨ì´ë‹¤. ë§ˆì§€ë§‰ì— ë‹¨ì¼ ë¦¬í„´ì´ë©´ `results[0]`ë§Œ êº¼ë‚´ê³ , ë‹¤ì¤‘ ë¦¬í„´ì´ë©´ íŠœí”Œ ê·¸ëŒ€ë¡œ ëŒë ¤ì¤€ë‹¤.

#### 8ï¸âƒ£ Backward í´ë¡œì ¸ êµ¬ì„± â€“ Gradientì˜ ì‹¤ì œ íë¦„

ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„ì€ backward ì‹œ í˜¸ì¶œë  _closure_ ì„ ë§Œë“œëŠ” ë‹¨ê³„ì´ë‹¤. ì´ ë¶€ë¶„ì´ì•¼ë§ë¡œ **operation abstraction**ì˜ í•µì‹¬ì´ë¼ê³  í•´ë„ ê³¼ì–¸ì´ ì•„ë‹ˆë‹¤.

```python
def wrapper(op_self: operation, *args, **kwargs) -> Tensor | tuple[Tensor, ...]:
	...
   	for result, compute_grad in func_return_pairs:
    	...
        def _backward_op(*, _func: Callable = compute_grad) -> None:
        	grads = _func()
            if n_in == 1 or not isinstance(grads, tuple):
            	grads = (grads,)
            
            if len(grads) != len(tensors):
            	# ë¦¬í„´ëœ ë¯¸ë¶„ê°’ì˜ ê°œìˆ˜ê°€ ì…ë ¥ í…ì„œ ì¸ì ìˆ˜ì™€ ì•ˆë§ì„ ë•Œ
            	raise ValueError(...)
            
            for tensor, grad in zip(tensors, grads):
            	new_grad = lucid._match_grad_shape(tensor.data, grad)
                lucid._set_tensor_grad(tensor, new_grad)
            
            return
        ...
```

ì´ í•¨ìˆ˜ëŠ” ë‚˜ì¤‘ì— `Tensor`ì˜ `backward` ë£¨í‹´ì—ì„œ í˜¸ì¶œëœë‹¤. ë‚´ë¶€ì—ì„œ í•˜ëŠ” ì¼ì€ ë‹¤ìŒê³¼ ê°™ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì½íŒë‹¤.

**1.** ë¨¼ì € ì—°ì‚° êµ¬í˜„ìê°€ ì •ì˜í•œ `compute_grad()`ë¥¼ í˜¸ì¶œí•´ _ì›ì‹œ gradient_ ë¥¼ ì–»ëŠ”ë‹¤. (`op_self.forward`ê°€ ë¦¬í„´í•œ ë¯¸ë¶„ ë©”ì„œë“œë¥¼ ì§€ì¹­)

**2.** Unary ì—°ì‚°ì˜ ê²½ìš°ë¥¼ ìœ„í•´ gradientê°€ ë‹¨ì¼ ê°’ì´ë©´ 1-tupleë¡œ ê°ì‹¼ë‹¤.

**3.** ì…ë ¥ Tensorë“¤ê³¼ gradientë¥¼ ë‚˜ë€íˆ `zip`ìœ¼ë¡œ ë¬¶ì–´ ìˆœíšŒí•œë‹¤.

**4.** `_match_grad_shape`ë¡œ _broadcasting/collpase_ ë¡œ ì¸í•´ ë³€í•œ gradientì˜ shapeë¥¼ ì›ë˜ ì…ë ¥ Tensorì˜ shapeì— ë§ê²Œ ë³€í™”ì‹œí‚¨ë‹¤.

**5.** ë§ˆì§€ë§‰ìœ¼ë¡œ `_set_tensor_grad`ë¥¼ í†µí•´ í•´ë‹¹ Tensorì˜ grad í•„ë“œì— gradientë¥¼ ëˆ„ì (accumulate)í•œë‹¤.

ì—¬ê¸°ì„œ ì¤‘ìš”í•œ ê²ƒì€, ì—°ì‚° êµ¬í˜„ìëŠ” gradientê°€ **ìˆ˜í•™ì ìœ¼ë¡œ ì–´ë–¤ ê°’ì´ì–´ì•¼ í•˜ëŠ”ì§€ë§Œ ì •ì˜**í•˜ë©´ ë˜ê³ , ê·¸ gradientë¥¼ ì–´ë–¤ ìˆœì„œë¡œ ì–´ë–¤ Tensorì— ëˆ„ì í•´ì•¼ í•˜ëŠ”ì§€ëŠ” decoratorê°€ ì „ë‹´í•œë‹¤ëŠ” ì ì´ë‹¤.

ì°¸ê³ ë¡œ `lucid._set_tensor_grad` í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬í˜„ë˜ì–´ ìˆë‹¤.

```python
def _set_tensor_grad(tensor: Tensor, grad: _NumPyArray, at: SupportsIndex = ...) -> None:
    if not tensor.requires_grad:
        return
    if tensor.grad is None:
        tensor.grad = grad  # ë¯¸ë¶„ê°’ ìµœì´ˆ ëˆ„ì 
    else:
    	# np.ndarray íƒ€ì…ì˜ tensor.gradê°€ read-onlyì¸ ê²½ìš°
        # ê°’ ë³µì œë¥¼ í†µí•´ ê°•ì œë¡œ writable ìƒíƒœë¡œ ì „í™˜
        if not tensor.grad.flags.writeable:
            tensor.grad = tensor.grad.copy()
		
        if tensor.grad.ndim == 0:
        	# ë¯¸ë¶„ê°’ì´ ìŠ¤ì¹¼ë¼ì¸ ê²½ìš° ë‹¨ìˆœ ë§ì…ˆ
            tensor.grad += grad
        else:
        	# ë¯¸ë¶„ê°’ì´ ë‹¤ì°¨ì› ë°°ì—´ì¸ ê²½ìš° ì¸ë±ì‹± ë„ì…
            tensor.grad[at] = tensor.grad[at] + grad
```

ê·¸ë¦¬ê³  gradientì˜ shapeì„ ì›ë˜ Tensorì˜ shapeê³¼ ë§ì¶°ì£¼ëŠ” ì¤‘ìš”í•œ ì—­í• ì„ í•˜ëŠ” `lucid._match_grad_shape`ëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬í˜„ë˜ì–´ ìˆë‹¤.

```python
def _match_grad_shape(data: _NumPyArray, grad: _NumPyArray) -> _NumPyArray:
	# data: ì›ë³¸ Tensorì˜ ë°ì´í„° ê°’
    # grad: ë§ì¶”ê³ ì í•˜ëŠ” gradient ê°’

    # (1) dataì™€ gradì˜ shapeê°€ ì´ë¯¸ ë™ì¼í•œ ê²½ìš° -> ê·¸ëŒ€ë¡œ ë°˜í™˜
    if data.shape == grad.shape:
        return grad

    # (2) dataê°€ ìŠ¤ì¹¼ë¼ì˜€ë˜ ê²½ìš°
    #     forwardì—ì„œ ìŠ¤ì¹¼ë¼ì™€ broadcastê°€ ì¼ì–´ë‚¬ë‹¤ë©´ gradëŠ” ì—¬ëŸ¬ ê°’ìœ¼ë¡œ í™•ì¥ë˜ì—ˆì„ ê²ƒ.
    #     ìŠ¤ì¹¼ë¼ì— ëŒ€í•œ gradëŠ” ê·¸ ëª¨ë“  ìš”ì†Œë¥¼ ë”í•œ ê°’ì´ë¯€ë¡œ sum()ìœ¼ë¡œ ì²˜ë¦¬.
    if data.ndim == 0:
        return grad.sum()

    # (3) gradê°€ ìŠ¤ì¹¼ë¼ì¸ ê²½ìš°
    #     forwardì—ì„œ dataê°€ gradë³´ë‹¤ ë” í° shapeì˜€ê³  broadcasting ë˜ì—ˆìŒ.
    #     gradë¥¼ data.shapeë¡œ broadcast ì‹œì¼œì•¼ í•¨.
    if grad.ndim == 0:
        return np.broadcast_to(grad, data.shape)

    # (4) dataì™€ gradì˜ ì´ element ìˆ˜(size)ê°€ ê°™ë‹¤ë©´
    #     shapeë§Œ ë‹¤ë¥´ê³  ë‚´ìš©ë¬¼ ìˆ˜ëŠ” ë™ì¼í•œ ê²½ìš° -> reshapeë¡œ í•´ê²° ê°€ëŠ¥
    if data.size == grad.size:
        return grad.reshape(data.shape)

    # (5) data.size > grad.size ì¸ ê²½ìš°
    #     ì¦‰, gradê°€ ë” ì‘ì€ í¬ê¸° -> forwardì—ì„œ gradê°€ ë°˜ë³µ(broadcast)ë˜ì–´ dataê°€ ìƒì„±ë¨.
    #     ë”°ë¼ì„œ gradë¥¼ í™•ì¥ì‹œì¼œì•¼ í•¨.
    elif data.size > grad.size:
        # gradë¥¼ flatten
        grad_squeeze = grad.flatten()
        # ì–¼ë§ˆë‚˜ ë°˜ë³µí•´ì•¼ í•˜ëŠ”ì§€ íšŸìˆ˜ ê³„ì‚°
        expand_factor = data.size / grad.size

        # ë°˜ë³µ íšŸìˆ˜ê°€ ì •ìˆ˜ê°€ ì•„ë‹ˆë¼ë©´ broadcastë¡œ ë§Œë“¤ ìˆ˜ ì—†ìŒ â†’ ì˜¤ë¥˜
        if expand_factor % 1 != 0:
            raise ValueError(
                f"Cannot broadcast grad of {grad.shape} to data of {data.shape}."
            )

        # gradë¥¼ expand_factorë§Œí¼ ë°˜ë³µí•˜ì—¬ í™•ì¥
        grad_expand = grad_squeeze[..., None].repeat(int(expand_factor), axis=-1)

        # ì›ë˜ dataì˜ shapeë¡œ reshape
        return grad_expand.reshape(data.shape)

    # (6) data.size < grad.size ì¸ ê²½ìš°
    #     ì¦‰, gradê°€ ë” í° í¬ê¸° -> forward ê³¼ì •ì—ì„œ dataê°€ í™•ì¥(broadcast)ë˜ë©´ì„œ gradê°€ ì¶•ì ë˜ì–´ì•¼ í•¨.
    #     ë°˜ëŒ€ë¡œ backpropì—ì„œëŠ” í™•ì¥ëœ gradë¥¼ collapse í•´ì•¼ í•¨ (sum ì¶•ì†Œ)
    elif data.size < grad.size:
        # grad.size ê°€ data.sizeë¡œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì§€ì§€ ì•Šìœ¼ë©´ collapse ë¶ˆê°€
        if grad.size % data.size != 0:
            raise ValueError(
                f"Cannot collapse grad of {grad.shape} to data of {data.shape}."
            )

        new_shape = tuple()
        remain_size = grad.size

        # data shapeì— ë§ê²Œ gradë¥¼ ì¬êµ¬ì„±í•  shapeì„ ê³„ì‚°
        for d_dim in data.shape:
            fac = remain_size // d_dim
            new_shape += (d_dim,)
            remain_size = fac

        # ë§ˆì§€ë§‰ ë‚¨ì€ ì°¨ì›ì„ ì¶”ê°€
        new_shape += (fac,)

        # collapseë¥¼ ìœ„í•´ ë§ˆì§€ë§‰ axisë¥¼ sumí•˜ì—¬ data.shapeë¡œ ì¶•ì†Œ
        return grad.reshape(new_shape).sum(axis=-1)

    # (7) ì´ ì™¸ì˜ ê²½ìš° â†’ ì²˜ë¦¬í•  ìˆ˜ ì—†ëŠ” ì˜ˆì™¸ ìƒí™©
    else:
        raise ValueError("Unknown error occurred.")
```

ì´ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ì˜ ë™ì‘ì„ ê°„ë‹¨í•˜ê²Œ í‘œë¡œ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

| ìƒí™© | ì˜ë¯¸ | ì²˜ë¦¬ ë°©ì‹ |
|------|------|-----------|
| **`shape` ë™ì¼** | ì´ë¯¸ ì›í•˜ëŠ” ëª¨ì–‘ | ê·¸ëŒ€ë¡œ ë°˜í™˜ |
| **`data`ê°€ ìŠ¤ì¹¼ë¼** | ëª¨ë“  `grad` ìš”ì†Œ í•©ì´ ìƒˆë¡œìš´ `grad` | `sum()` |
| **`grad`ê°€ ìŠ¤ì¹¼ë¼** | ìŠ¤ì¹¼ë¼ $\rightarrow$ `data.shape` ë¡œ broadcast | `broadcast_to()` |
| **`size` ë™ì¼** | shapeë§Œ ë‹¤ë¦„ | `reshape()` |
| **`data.size > grad.size`** | `grad`ê°€ ë°˜ë³µë˜ì–´ broadcast | `grad`ë¥¼ repeatí•´ì„œ í™•ì¥ |
| **`data.size < grad.size`** | `grad`ê°€ broadcast í›„ ì¶•ì ë¨ | `grad`ë¥¼ reshape í›„ `sum()`ìœ¼ë¡œ collapse |

ìƒê°ë³´ë‹¤ ì—°ì‚° ê³¼ì •ì„ ê±°ì¹˜ë©´ì„œ Tensorì˜ shapeì´ broadcast/collapseì— ì˜í•´ ë³€í•˜ëŠ” **ê²½ìš°ì˜ ìˆ˜ê°€ ë§ì•„** ì´ í•¨ìˆ˜ì˜ ë¡œì§ì„ êµ¬í˜„í•˜ëŠ” ë° ìˆì–´ì„œ ì ì§€ ì•Šì€ ì–´ë ¤ì›€ì„ ëŠê¼ˆë‹¤.

#### 9ï¸âƒ£ ê²°ê³¼ Tensorì— Backward ì—°ê²°ê³¼ ìµœì¢… ë¦¬í„´

ë§ˆì§€ë§‰ìœ¼ë¡œ gradient ì¶”ì  ëŒ€ìƒì¸ ê²°ê³¼ Tensorì— ìœ„ì˜ **bakcward closure** `_backward_op`ë¥¼ ì‹¤ì œë¡œ ì—°ê²°í•˜ê³ , ë¦¬í„´ ê°’ì„ ì •ë¦¬í•˜ëŠ” ë‹¨ê³„ê°€ ì˜¨ë‹¤.

```python
def wrapper(op_self: operation, *args, **kwargs) -> Tensor | tuple[Tensor, ...]:
	...
    for result, compute_grad in func_return_pairs:
    	...
        if result.requires_grad:
        	result._prev = list(tensors)
            result._backward_op = _backward_op
        
    return results if num_returns > 1 else results[0]
```

`result.requires_grad`ê°€ `True`ì¼ ë•Œë§Œ `_prev`ì™€ `_backward_op`ë¥¼ ì„¤ì •í•˜ëŠ” ì´ìœ ëŠ”, gradientë¥¼ ì¶”ì í•˜ì§€ ì•Šì•„ë„ ë˜ëŠ” Tensorì— ëŒ€í•´ êµ³ì´ ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ìœ ì§€í•  í•„ìš”ëŠ” ì—†ê¸° ë•Œë¬¸ì´ë‹¤. ì´ë ‡ê²Œ í•¨ìœ¼ë¡œì¨ **ë¶ˆí•„ìš”í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ì¤„ì´ê³ **, backward ì‹œì—ë„ ì“¸ë°ì—†ëŠ” ê²½ë¡œë¥¼ ë”°ë¼ê°€ì§€ ì•Šê²Œ ëœë‹¤.

ì´ í•œ ì¤„, `result._prev = list(tensors)`ëŠ” ì´ ê²°ê³¼ Tensorê°€ ê·¸ë˜í”„ ìƒì—ì„œ ì–´ë””ì—ì„œ ì™”ëŠ”ì§€ë¥¼ ê¸°ë¡í•˜ëŠ” **í•µì‹¬ ì—°ê²° ê³ ë¦¬**ì´ë‹¤. ë°˜ëŒ€ë¡œ `_backward_op`ëŠ” ì´ Tensorë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì–´ë–»ê²Œ ë¶€ëª¨ë“¤ì—ê²Œ gradientë¥¼ ëŒë ¤ì¤„ ê²ƒì¸ì§€ë¥¼ _encapsulate_ í•œ í•¨ìˆ˜ í¬ì¸í„° ì—­í• ì„ í•œë‹¤.

ì´ ë‘˜ì´ ë¬¶ì—¬ ìˆëŠ” êµ¬ì¡°ê°€ ë°”ë¡œ Lucidì˜ autodiff ê·¸ë˜í”„ë¥¼ ì´ë£¨ëŠ” **ê¸°ë³¸ ë‹¨ìœ„**ì´ë‹¤.

ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ `return` ë¬¸ì—ì„œ ë‹¨ì¼ ë¦¬í„´ê³¼ ë‹¤ì¤‘ ë¦¬í„´ì„ êµ¬ë¶„í•´ ì ì ˆí•œ í˜•íƒœë¡œ ëŒë ¤ì¤Œìœ¼ë¡œì¨, ì‚¬ìš©ì ì…ì¥ì—ì„œëŠ” **ìì—°ìŠ¤ëŸ¬ìš´ Python í•¨ìˆ˜ì²˜ëŸ¼** ì—°ì‚°ì„ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ëœë‹¤.

### ğŸ `@func_op`ì˜ íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ Wrapping

ì‚¬ì‹¤ ëŒ€ë¶€ë¶„ì˜ ì—°ì‚°ì€ ì…ë ¥ì´ 1ê°œê±°ë‚˜ 2ê°œì˜€ë‹¤. ë‹¤ë¥¸ ë§ë¡œ í•˜ìë©´ ëŒ€ë¶€ë¶„ì˜ ì—°ì‚°ì€ **unary ì´ê±°ë‚˜ binary ì—°ì‚°**ì´ë¼ëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œ ì´ë¥¼ ë” í¸í•˜ê²Œ ì“°ê¸° ìœ„í•´ _ì–‡ì€ wrapper_ ë“¤ì„ ë§Œë“¤ì—ˆë‹¤.

```python
def unary_func_op(has_gradient: bool = True) -> Callable:
	# n_in=1, n_ret=1 ê³ ì •
    return func_op(1, 1, has_gradient=has_gradient)


def binary_func_op(has_gradient: bool = True) -> Callable:
	# n_in=2, n_ret=1 ê³ ì •
    return func_op(2, 1, has_gradient=has_gradient)
```

ì¶”ê°€ì ìœ¼ë¡œ ì…ë ¥ì´ $n$ê°œì¸ **polyadic ì—°ì‚°**ì„ ìœ„í•œ wrapper ë˜í•œ ë§Œë“¤ì—ˆë‹¤.

```python
def poly_func_op(has_gradient: bool = True) -> Callable:
	# n_inì€ ë™ì  ì¶”ë¡ , n_ret=1 ê³ ì •
    return func_op(None, 1, has_gradient=has_gradient)
```

ê²°êµ­ ì´ helperë“¤ì€ ì—°ì‚° êµ¬í˜„ìê°€ ë¶ˆí•„ìš”í•œ ê³ ë¯¼ì„ í•˜ì§€ ì•Šê²Œ ë˜ê³ , ì—°ì‚° ì •ì˜ë¥¼ ë”ìš± ê°€ë…ì„± ì¢‹ê³  ìœ ì§€ë³´ìˆ˜í•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ì •ë¦¬í•˜ê¸° ìœ„í•œ ì¥ì¹˜ì´ë‹¤.

---

### ğŸª„ `mul` ì˜ˆì‹œë¥¼ í†µí•´ ì „ ê³¼ì • ì‚´í´ë³´ê¸°

ì´ì œ ì´ë ‡ê²Œ êµ¬ì¶•ëœ Lucidì˜ autodiff ì—”ì§„ ê¸°ë°˜ì˜ Tensor operation ì‹œìŠ¤í…œì„ ì‹¤ì œ `mul` ì—°ì‚° ì˜ˆì‹œë¥¼ í†µí•´ ì²˜ìŒë¶€í„° ë¹ ë¥´ê²Œ í›‘ì–´ë³´ì.

#### 0ï¸âƒ£ í…ì„œ ì„¸íŒ…ê³¼ ì›ì†Œë³„ ê³±ì…ˆ ì‹¤í–‰

```python
a = lucid.Tensor([1, 2], requires_grad=True)
b = lucid.Tensor([3, 4], requires_grad=True)

c = a * b  # ì›ì†Œë³„ ê³±ì…ˆ í˜¸ì¶œ
```

ì‹¤ì œë¡œëŠ” í¸ì˜ë¥¼ ìœ„í•´ Lucid ìƒì˜ ëª¨ë“  Tensor operationë“¤ì€ ë‹¤ìŒê³¼ ê°™ì´ **ìƒìœ„ namespaceì—ì„œ ë³„ë„ì˜ í•¨ìˆ˜ë¡œ wrapping** ë˜ì–´ìˆë‹¤. ì¦‰, ê°™ì€ ë§ì…ˆ ì—°ì‚°ì´ì–´ë„ ë§¤ í˜¸ì¶œë§ˆë‹¤ ë‹¤ë¥¸ `mul` í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ê°€ ìƒì„±ë˜ëŠ” ê²ƒì´ë‹¤. ì´ë ‡ê²Œ í•¨ìœ¼ë¡œì¨ ê·¸ë˜í”„ ìƒì—ì„œì˜ ì—°ì‚°ì˜ ì—­ì¶”ì ì´ ê¼¬ì´ì§€ ì•Šê²Œ í•  ìˆ˜ ìˆë‹¤.

```python
# lucid/_func/__init__.py
from lucid._func.bfunc import mul

def mul(a: Tensor, b: Tensor) -> Tensor:
	return mul()(a, b)  # ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í›„ í˜¸ì¶œ(mul.__call__)
```

ê·¸ë¦¬ê³  ì´ `mul` wrapping í•¨ìˆ˜ëŠ” `Tensor` í´ë˜ìŠ¤ì˜ ê¸°ë³¸ `*` ì—°ì‚°ì ë©”ì†Œë“œì¸ `__mul__`ì„ _override(ì˜¤ë²„ë¼ì´ë“œ)_ í•œë‹¤.

```python
# lucid/_tensor/tensor.py
from .tensor_base import _TensorBase

class Tensor(_TensorBase): ...
```

```python
# lucid/_tesnor/tensor_base.py
from typing import Self

class _TensorBase:  # Tensor ìƒìœ„ í´ë˜ìŠ¤
	...
    # ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ë§Œ ì •ì˜
    def __mul__(self, other: Self) -> Self: ...
    ...
```

```python
# lucid/_func/__init__.py

from lucid._tensor import Tensor

Tensor.__mul__ = mul  # ë™ì (ëŸ°íƒ€ì„) ë©”ì„œë“œ ì˜¤ë²„ë¼ì´ë”©
```

ì´ë•Œ, `operation`ì˜ í•˜ìœ„ í´ë˜ìŠ¤ì¸ `mul`ì€ ë‹¤ìŒê³¼ ê°™ì´ êµ¬í˜„ë˜ì–´ ìˆë‹¤.

```python
# lucid/_func/bfunc.py

class mul(operation):
	def __init__(self) -> None:
    	super().__init__()
    
    @binary_func_op()
    def forward(self, a: Tensor, b: Tesnsor) -> _FuncOpReturnType:
    	self.result = Tensor(a.data * b.data)
        return self.result, self.backward
    
    def backward(self) -> _GradFuncType:
    	return (
        	self.result.grad * b.data,  # d(a*b)/da = b
            self.result.grad * a.data,  # d(a*b)/db = a
        )
```

ì¦‰, ì˜ˆì‹œ ì½”ë“œì—ì„œ `c = a * b`ë¥¼ ì‹¤í–‰í•˜ë©´

**1.** `*` ì—°ì‚°ìê°€ `a.__mul__`ì„ ì ‘ê·¼í•˜ê²Œ ë˜ê³  ì´ëŠ” ì˜¤ë²„ë¼ì´ë”©ëœ `lucid._func.mul` í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ê²Œ ëœë‹¤.

**2.** ê·¸ëŸ¬ë©´ `lucid._func.bfunc.mul` í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ê²Œ ë˜ê³ , `mul()(a, b)`ì—ì„œ `mul.__call__`ì˜ ì§ì ‘ í˜¸ì¶œì„ í†µí•´ `mul.forward(a, b)`ê°€ í˜¸ì¶œëœë‹¤.

#### 1ï¸âƒ£ `@binary_func_op` í˜¸ì¶œ

`mul.forward`ê°€ ì‹¤í–‰ë˜ê¸° ì „, ë°ì½”ë ˆì´í„° íŒ©í† ë¦¬ wrapper `@binary_func_op()`ì— ì˜í•´ ë°ì½”ë ˆì´í„° íŒ©í† ë¦¬ `@func_op()`ì´ í˜¸ì¶œëœë‹¤.

```python
def binary_func_op(...) -> Callable:
	return func_op(n_in=2, n_ret=1, ...)
```

#### 2ï¸âƒ£ `a`, `b` íƒ€ì… ê²€ì‚¬ ë° ìŠ¹ê²©

ì´ì œ `@func_op` ë‚´ë¶€ ë¡œì§ì— ë”°ë¼ ì—°ì‚°ì˜ ì…ë ¥ ì¸ìì¸ `a`, `b`ì— ëŒ€í•´ `Tensor` ì¸ìŠ¤í„´ìŠ¤ì¸ì§€ **íƒ€ì… ê²€ì‚¬**ë¥¼ í•˜ê³  í•„ìš”ì— ë”°ë¼ ì ì ˆíˆ ìŠ¹ê²©ì„ ì§„í–‰í•œë‹¤.

í•˜ì§€ë§Œ ì´ ì˜ˆì‹œì—ì„œëŠ” `a`, `b`ëŠ” ì´ë¯¸ ëª¨ë‘ `Tensor` ì´ë¯€ë¡œ ë³„ë„ì˜ ìŠ¹ê²© ì—†ì´ ë„˜ì–´ê°„ë‹¤.

#### 3ï¸âƒ£ `mul.forward` í˜¸ì¶œ ë° ë¦¬í„´ íŠœí”Œ íšë“

ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ `@func_op`ì´ ì •ì œëœ `a`, `b`ë¥¼ `mul.forward`ì— ë„˜ê²¨ **ì‹¤ì œ ì›ì†Œë³„ ê³±**ì„ ì‹¤í–‰í•˜ê³ , ì´ë¡œë¶€í„° ê²°ê³¼ í…ì„œì™€ ê·¸ì— ë”°ë¥¸ backward ë©”ì„œë“œë¥¼ ë¦¬í„´ ë°›ëŠ”ë‹¤.

#### 4ï¸âƒ£ ê²°ê³¼ Tensor í›„ì²˜ë¦¬ ë° ìµœì¢… ë¦¬í„´

ì´ì œ `@func_op`ì´ ë³¸ì¸ì˜ ë§ˆì§€ë§‰ ê³¼ì •ì¸ ê°ì¢… **computation graph ê´€ë ¨ ë©”íƒ€ë°ì´í„°**ë“¤ì„ ê²°ê³¼ Tensorì— ì§€ì •í•œë‹¤.

- `result.requires_grad`: `a`, `b` ëª¨ë‘ `True` ì´ë¯€ë¡œ `True`
- `result._prev`: ì—°ì‚°ì˜ ì…ë ¥ ì¸ìì¸ `a`, `b`ë¥¼ ë¶€ëª¨ë¡œ ì§€ì •
- `result._backward_op`: `@func_op` ë‚´ë¶€ì—ì„œ ë™ì ìœ¼ë¡œ ìƒì„±í•œ closureì¸ `_backward_op`ë¡œ ë°”ì¸ë”©, ì´ë•Œ closureì˜ `_func` ì¸ìëŠ” `mul.backward` ì´ë‹¤.

ì´ëŸ¬í•œ í›„ì²˜ë¦¬ê°€ ëª¨ë‘ ì™„ë£Œë˜ë©´ `result` í…ì„œë¥¼ ìµœì¢…ì ìœ¼ë¡œ ë¦¬í„´í•˜ì—¬ `c = a * b`ì˜ ì‹¤í–‰ì„ ë¯¸ë¬´ë¦¬í•œë‹¤.

#### â‡ï¸ ê²°ê³¼ í…ì„œì— ëŒ€í•œ ì—­ì „íŒŒ ìˆ˜í–‰

ì´ì œ forwardê°€ ëë‚¬ìœ¼ë‹ˆ `c.backward()`ë¥¼ í˜¸ì¶œí•˜ì—¬ $\partial \mathbf{c}/\partial\mathbf{a}$ì™€ $\partial \mathbf{c}/\partial\mathbf{b}$ë¥¼ êµ¬í•´ë³´ì.

```python
c.backward()
```

#### 5ï¸âƒ£ `c.grad` ì´ˆê¸°í™”

`c` í…ì„œëŠ” ì•„ë¬´ëŸ° gradient ê°’ì„ ê°€ì§€ê³  ìˆì§€ ì•Šìœ¼ë¯€ë¡œ, `c.backward` ë‚´ë¶€ì—ì„œ ìµœì´ˆ gradientì„ $\mathbf{1}\in\mathbb{R}^2$ë¡œ ì´ˆê¸°í™”í•œë‹¤.

```python
c.grad = None -> np.ones_like(c.data)
```

#### 6ï¸âƒ£ ì—­ì „íŒŒ ìˆœì„œ ì„¤ì •ì„ ìœ„í•œ ìœ„ìƒ ì •ë ¬ ìˆ˜í–‰

ê° í…ì„œì˜ `backward`ë¥¼ í˜¸ì¶œí•˜ê¸° ìœ„í•œ ìˆœì„œë¥¼ ì •í•˜ê¸° ìœ„í•´ `c._prev`ì—ì„œ ì‹œì‘í•˜ì—¬ **ìœ„ìƒ ì •ë ¬(topological sort)**ë¥¼ ìˆ˜í–‰í•œë‹¤.

ê·¸ì— ë”°ë¥¸ ê²°ê³¼ëŠ” `(a, b, c)`ê°€ ë˜ê³ , ë”°ë¼ì„œ ì‹¤ì œ backward í˜¸ì¶œ ìˆœì„œëŠ” ì´ì˜ _ì—­ìˆœ_ ì¸ `(c, b, a)`ê°€ ëœë‹¤.

ì´ì œ ì´ ìˆœì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ê° í…ì„œì˜ `op._backward_op`ì´ í˜¸ì¶œë˜ê²Œ ëœë‹¤.

#### 7ï¸âƒ£ `@func_op`ì— ì˜í•´ ë°”ì¸ë”©ëœ `c._backward_op` ì‹¤í–‰

ì•ì„œ ë°ì½”ë ˆì´í„° `@func_op`ì— ì˜í•´ ë°”ì¸ë”©ëœ closure `_backward_op`ì´ ì‹¤í–‰ëœë‹¤.

```python
def _backward_op(*, _func: Callable = mul.backward) -> None:
        grads = _func() # grad_a, grad_bê°€ ë‹´ê¸´ íŠœí”Œ
        ...
        for tensor, grad in zip(tensors, grads):
            new_grad = lucid._match_grad_shape(tensor.data, grad)
            lucid._set_tensor_grad(tensor, new_grad)
```

`mul`ì—°ì‚°ì€ ë‘ ê°œì˜ í…ì„œ ì…ë ¥ì„ ë°›ìœ¼ë¯€ë¡œ, `_func` ì¦‰, `mul.backward`ëŠ” ë‹¤ìŒì˜ **ë‘ gradientê°€ ë‹´ê¸´ íŠœí”Œ** $(\partial \mathbf{c}/\partial\mathbf{a},~\partial \mathbf{c}/\partial\mathbf{b})$ì„ ë¦¬í„´í•œë‹¤.

ì´í›„, ê° gradientì˜ shapeì´ ê° input í…ì„œ `a`, `b`ì˜ shapeê³¼ ë§ëŠ”ì§€ í™•ì¸í•˜ê³  ì´ìƒì´ ì—†ìœ¼ë©´ `a.grad`, `b.grad` ê°ê°ì— ëŒ€ì‘í•˜ëŠ” gradient ê°’ì„ ëˆ„ì í•œë‹¤.

#### 8ï¸âƒ£ Non-leaf í…ì„œë“¤ì˜ Gradient íšŒìˆ˜

ì‹¤ì§ˆì ì¸ gradient ì „íŒŒê°€ ëª¨ë‘ ì™„ë£Œë˜ì—ˆìœ¼ë¯€ë¡œ, ì¤‘ê°„ ë‹¨ê³„ì˜ non-leaf í…ì„œë“¤ì´ ê°€ì§€ê³  ìˆëŠ” gradientë¥¼ ë‹¤ì‹œ `None`ìœ¼ë¡œ **ì´ˆê¸°í™”**í•œë‹¤.

ì´ ì˜ˆì‹œì—ì„œëŠ” `a`ì™€ `b` ëª¨ë‘ **gradient ì¶”ì  ëŒ€ìƒì„ê³¼ ë™ì‹œì— ë§ë‹¨ ë…¸ë“œ(terminal node)** ì´ê¸° ë•Œë¬¸ì— ì´ë¥¼ ì œì™¸í•œ `c`ì˜ gradientë§Œ íšŒìˆ˜ëœë‹¤(`c.grad = None`).

#### âœ… ìµœì¢… ê²°ê³¼ í™•ì¸

ì´ì œ `a`, `b`, ê·¸ë¦¬ê³  `c` í…ì„œë¥¼ ì¶œë ¥í•´ë³´ì.

```python
Tensor([1 2], grad=[3. 4.])  # a
Tensor([3 4], grad=[1. 2.])  # b
Tensor([3 8], grad=None)  # c
```

ì´ë¡œì¨ ê°„ë‹¨í•œ forward/backward ìŠ¤ìœ™ì„ í†µí•´ Lucidì˜ í•µì‹¬ì ì¸ autodiff ì—”ì§„ ê¸°ë°˜ì˜ Tensor operation ì‹œìŠ¤í…œì´ ì–´ë–¤ ì‹ìœ¼ë¡œ êµ¬ë™ë˜ëŠ”ì§€ ì•Œì•„ë³´ì•˜ë‹¤.

---

### ğŸŒ™ Lucidì˜ ì—°ì‚° ì²´ê³„ê°€ í™•ë¦½ë˜ë‹¤

ì´ë ‡ê²Œ íƒ„ìƒí•œ `@func_op` ê¸°ë°˜ operation ì‹œìŠ¤í…œì€ ë‹¨ìˆœí•œ ê¸°ìˆ ì  ì¥ì¹˜ê°€ ì•„ë‹ˆë¼ **Lucid ì „ì²´ì˜ ì² í•™**ì„ ë‹´ê³  ìˆë‹¤. ì—°ì‚° êµ¬í˜„ìëŠ” ìˆ˜í•™ì  ì •ì˜ì—ë§Œ ì§‘ì¤‘í•˜ê³ , ê·¸ë˜í”„ ì—°ê²°ê³¼ gradient ì—­ì „íŒŒëŠ” ìë™í™”ëœë‹¤. `Tensor`ëŠ” ë°ì´í„°ë¥¼ ë‹´ëŠ” _ê·¸ë¦‡_ ì´ê³ , `operation`ì€ ê·¸ë¦‡ì„ _ë³€í˜•ì‹œí‚¤ëŠ” ê·œì¹™_ ì´ë©°, `@func_op`ì€ ê·¸ ê³¼ì •ì´ ê·¸ë˜í”„ ìœ„ì—ì„œ _ì¼ê´€ëœ íë¦„ìœ¼ë¡œ ì´ì–´ì§€ë„ë¡_ í•œë‹¤.

ì´ êµ¬ì¡°ê°€ ì™„ì„±ë˜ì—ˆì„ ë•Œ ë‚˜ëŠ” ë¹„ë¡œì†Œ Lucidê°€ autodiff ì—”ì§„ìœ¼ë¡œì„œ **í•˜ë‚˜ì˜ ìœ ê¸°ì²´ì²˜ëŸ¼** ì›€ì§ì´ê¸° ì‹œì‘í–ˆë‹¤ëŠ” ëŠë‚Œì„ ë°›ì•˜ë‹¤. ì—°ì‚°ë“¤ì€ ë” ì´ìƒ ì œê°ê° í©ì–´ì ¸ ìˆëŠ” í•¨ìˆ˜ê°€ ì•„ë‹ˆë¼, ê³µí†µëœ ì›ë¦¬ì™€ ì¼ê´€ëœ êµ¬ì¡° ì†ì—ì„œ ì´ì–´ì§„ ì‘ì€ ë…¸ë“œë“¤ì˜ ì—°ì†ì´ ë˜ì—ˆë‹¤.

ë‹¤ìŒ ê°œë°œ ì¼ì§€ì—ì„œëŠ” ë°”ë¡œ ì´ abstraction ìœ„ì—ì„œ **ê°œë³„ ì—°ì‚°ë“¤ì„ ì–´ë–»ê²Œ êµ¬í˜„**í–ˆëŠ”ì§€, íŠ¹íˆ `add`, `matmul` ê°™ì€ ì´ˆê¸° ì—°ì‚°ë“¤ì˜ êµ¬í˜„ ê³¼ì •ê³¼ ê·¸ ê³¼ì •ì—ì„œ ë§ˆì£¼ì³¤ë˜ gradient ë²„ê·¸ë“¤ì„ ì–´ë–»ê²Œ í•˜ë‚˜ì”© ë‹¤ë¤„ê°”ëŠ”ì§€ë¥´ ì´ì–´ì„œ ê¸°ë¡í•  ê²ƒì´ë‹¤.