## ğŸ§± Functional API êµ¬ì¶•

### ğŸ§­ ê³„ì¸µì  ì¶”ìƒí™” ì„¤ê³„

Lucidë¥¼ ì„¤ê³„í•  ë•Œ ê°€ì¥ ë¨¼ì € ë°•ì€ ë§ëšì€ **ê³„ì¸µë³„ë¡œ ì±…ì„ì„ ë¶„ë¦¬**í•œë‹¤ëŠ” ì›ì¹™ì´ì—ˆë‹¤. ì•„ë˜ì™€ ê°™ì€ íë¦„ì„ ì—„ê²©í•˜ê²Œ ìœ ì§€í•˜ë ¤ í–ˆë‹¤.

1. **Tensor**: ë°ì´í„°/gradientë¥¼ ë“¤ê³  ìˆëŠ” ìµœí•˜ìœ„ ë…¸ë“œ.
2. **ê¸°ë³¸ ì—°ì‚°**: `add`, `mul`, `matmul` ë“± ìˆ˜í•™ì  primitive. ì—¬ê¸°ì„œë§Œ NumPyê°€ ì§ì ‘ í˜¸ì¶œëœë‹¤.
3. **`nn.functional`**: ì„ í˜• ë³€í™˜, í™œì„±í™”, í’€ë§, ì •ê·œí™”, ë“œë¡­ì•„ì›ƒ, ì†ì‹¤ ë“± ìƒíƒœ ì—†ëŠ” ì—°ì‚°ì„ primitive ì¡°í•©ìœ¼ë¡œ ì •ì˜í•œë‹¤.
4. **`nn.Module`**: `Conv2d`, `Linear`, `BatchNorm` ë“± ìƒíƒœ(state)ë¥¼ ê°€ì§„ ëª¨ë“ˆ.
5. **ëª¨ë¸**: LeNet, AlexNet ë“± ì‹¤ì œ ë„¤íŠ¸ì›Œí¬ë¥¼ ëª¨ë“ˆ ì¡°í•©ìœ¼ë¡œ ì •ì˜.

ì´ ê³„ì¸µì„ ì§€í‚¤ë©´ **ìœ ì§€ë³´ìˆ˜ì„±**ê³¼ **ì‹œìŠ¤í…œì  ê°€ë…ì„±**ì´ í™•ë³´ëœë‹¤. íŠ¹íˆ NumPy í˜¸ì¶œì´ 2ë‹¨ê³„(ê¸°ë³¸ ì—°ì‚°)ì—ì„œë§Œ ë°œìƒí•˜ë„ë¡ ë´‰ì¸í•˜ë©´, ìƒìœ„ ë ˆì´ì–´ëŠ” ëª¨ë‘ ë™ì¼í•œ ì¶”ìƒí™” ê·œì¹™ì— ì˜ì¡´í•˜ê²Œ ëœë‹¤. functional ê³„ì¸µì„ ë§Œë“¤ë©´ì„œë„ ì´ ê·œìœ¨ì„ ë¬´ì¡°ê±´ ì§€í‚¤ëŠ” ê²ƒì„ ëª©í‘œë¡œ ì‚¼ì•˜ë‹¤. ê¸°ëŠ¥ë³„ APIë¥¼ ì‘ì„±í•  ë•Œë§ˆë‹¤ â€œì´ ì½”ë“œê°€ primitive ì™¸ë¶€ í˜¸ì¶œì„ ì„ì§€ ì•ŠëŠ”ê°€?â€, â€œëª¨ë“ˆì´ ê°€ì ¸ê°€ì•¼ í•  ìƒíƒœë¥¼ functionalì— ìˆ¨ê¸°ì§€ ì•Šì•˜ëŠ”ê°€?â€ ê°™ì€ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜ë³µí–ˆë‹¤.

---

### ğŸ§© `nn.functional`ì˜ ëª©í‘œì™€ ì œì•½

`nn.functional`ì€ ìƒíƒœ ì—†ëŠ” ìˆœìˆ˜ ì—°ì‚° ì§‘í•©ì´ë‹¤. ì…ë ¥ Tensorì™€ íŒŒë¼ë¯¸í„° Tensorë¥¼ ë°›ì•„ ì¦‰ì‹œ ì¶œë ¥ Tensorë¥¼ ë°˜í™˜í•˜ë©°, **ìƒíƒœ ì €ì¥ì„ í•˜ì§€ ì•ŠëŠ”ë‹¤**. ë”°ë¼ì„œ
- **autograd**: gradient ì¶”ì ì€ ì´ë¯¸ í•˜ìœ„ ì—°ì‚°(`@func_op` ê¸°ë°˜)ì—ì„œ ì²˜ë¦¬ë˜ë¯€ë¡œ ë³„ë„ ë¡œì§ì´ í•„ìš” ì—†ë‹¤.
- **ë””ë°”ì´ìŠ¤ ì¢…ì†ì„± ìµœì†Œí™”**: NumPy ì™¸ë¶€ í˜¸ì¶œ ì—†ì´, ìƒìœ„ APIì—ì„œëŠ” ì˜¤ì§ í•˜ìœ„ primitive(`lucid.add`, `lucid.matmul`, `lucid.exp` ë“±)ë§Œ ì‚¬ìš©í•œë‹¤.
- **ì…ì¶œë ¥ ê·œì•½ ëª…í™•í™”**: broadcasting, shape ë³€í™˜, íŒ¨ë”©/ìŠ¤íŠ¸ë¼ì´ë“œ ê³„ì‚°ì„ í•¨ìˆ˜ ë‹¨ìœ„ë¡œ ìº¡ìŠí™”í•´ ëª¨ë“ˆ(`nn.Conv2d` ë“±)ì´ ê·¸ëŒ€ë¡œ ì¬ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ í•œë‹¤.

ì´ ëª©í‘œë¥¼ ì§€í‚¤ê¸° ìœ„í•´ ëŒ€í‘œ functional ì—°ì‚°ë“¤ì„ ì •ë¦¬í•œë‹¤. `relu`, `softmax`, `linear`, `max_pool`, `batch_norm`, `dropout`, `cross_entropy`ì²˜ëŸ¼ í•™ìŠµ ë£¨í”„ì—ì„œ ë°˜ë³µ í˜¸ì¶œë˜ëŠ” APIê°€ í•˜ìœ„ primitiveë§Œìœ¼ë¡œ ì–´ë–»ê²Œ êµ¬ì„±ë˜ëŠ”ì§€ê°€ í¬ì¸íŠ¸ë‹¤. (ì»¨ë³¼ë£¨ì…˜ì€ êµ¬í˜„ì´ ê¸¸ì–´ ë³„ë„ ë¬¸ì„œì—ì„œ ë‹¤ë£° ì˜ˆì •ì´ë‹¤.) ê° í•¨ìˆ˜ê°€ primitive í˜¸ì¶œë§Œìœ¼ë¡œ ì‘ì„±ë˜ë©´, ë°”ë¡œ ê·¸ ìœ„ ì¸µ(`nn.Module`)ì€ **ìƒíƒœ ë³´ê´€ + functional ìœ„ì„**ì´ë¼ëŠ” ì–‡ì€ ì—­í• ì— ì§‘ì¤‘í•  ìˆ˜ ìˆë‹¤.

### âš¡ `relu`: ë§ˆìŠ¤í¬ ê¸°ë°˜ ë¶„ê¸°

ê²½ë¡œ: [`lucid/nn/functional/_activation.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/nn/functional/_activation.py#L6-L16)

```python
def relu(input_: Tensor) -> Tensor:
    return lucid.maximum(0, input_)
```

- **ì˜ì¡´ì„±**: `maximum`(primitive)ë§Œ ì‚¬ìš©. NumPy í˜¸ì¶œ ì—†ìŒ.
- **ìˆ˜ì‹**: $f(x)=\max(0, x)$, $\partial f/\partial x = \mathbb{1}[x>0]$  
- **ì„¤ê³„ ë©”ëª¨**: ë¶„ê¸° ë§ˆìŠ¤í¬ë¥¼ primitiveê°€ ì²˜ë¦¬í•˜ë¯€ë¡œ `functional`ì—ì„œëŠ” ë¡œì§ ë¶„ê¸°ê°€ ì—†ë‹¤. ìƒìœ„ `nn.ReLU` ëª¨ë“ˆì€ ì´ í•¨ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ í˜¸ì¶œí•´ `forward`ë¥¼ êµ¬ì„±í•œë‹¤.

### ğŸŒ¡ï¸ `softmax`: ì•ˆì •í™”ì™€ ì¶• ê·œì•½

ê²½ë¡œ: [`lucid/nn/functional/_activation.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/nn/functional/_activation.py#L18-L36)

```python
def softmax(input_: Tensor, axis: int = -1) -> Tensor:
    input_max = lucid.max(input_, axis=axis, keepdims=True)
    input_stable = input_ - input_max
    e_input = lucid.exp(input_stable)
    sum_e_input = e_input.sum(axis=axis, keepdims=True)
    return e_input / sum_e_input
```

- **ì˜ì¡´ì„±**: `max`, `sub`, `exp`, `sum`, `truediv` ë“± ëª¨ë‘ primitive.  
- **ìˆ˜ì‹**: $\mathrm{softmax}(x)_i = \exp(x_i - \max x) / \sum_j \exp(x_j - \max x)$  
- **ì„¤ê³„ ë©”ëª¨**: ìˆ˜ì¹˜ ì•ˆì •í™”ë¥¼ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì²˜ë¦¬í•´, ìƒìœ„ ëª¨ë“ˆì´ë‚˜ loss êµ¬í˜„ì—ì„œ ë³„ë„ ì „ì²˜ë¦¬ë¥¼ í•˜ì§€ ì•Šì•„ë„ ëœë‹¤. gradientëŠ” í•˜ìœ„ ì—°ì‚°ë“¤ì´ ì¡°í•©ë¼ ìë™ ê³„ì‚°ëœë‹¤.

### ğŸ§® `linear`: í–‰ë ¬ ê³±ê³¼ í¸í–¥

ê²½ë¡œ: [`lucid/nn/functional/_linear.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/nn/functional/_linear.py#L6-L21)

```python
def linear(input_: Tensor, weight: Tensor, bias: Tensor | None = None) -> Tensor:
    output = input_ @ weight.mT
    if bias is not None:
        output += bias
    return output
```

- **ì˜ì¡´ì„±**: `matmul`(primitive)ê³¼ `add`.  
- **ìˆ˜ì‹**: $Y = X W^\top + b$  
- **ì„¤ê³„ ë©”ëª¨**: í¸í–¥ì€ broadcastingìœ¼ë¡œ ë”í•´ì§€ë¯€ë¡œ ë³„ë„ reshape ì—†ì´ primitive ê·œì•½ì— ë§¡ê¸´ë‹¤. ì´ í•¨ìˆ˜ê°€ `nn.Linear.forward`ì˜ ì „ë¶€ì´ë©°, íŒŒë¼ë¯¸í„°ëŠ” ëª¨ë“ˆì´ ì†Œìœ í•˜ê³  ì—°ì‚°ì€ `functional`ì´ ë‹´ë‹¹í•˜ëŠ” êµ¬ì¡°ê°€ ìœ ì§€ëœë‹¤.

### ğŸ§ª `batch_norm`: ëŸ¬ë‹ ìŠ¤íƒ¯ê³¼ ë¸Œë¡œë“œìºìŠ¤íŠ¸

ê²½ë¡œ: [`lucid/nn/functional/_norm.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/nn/functional/_norm.py#L13-L58)

```python
def batch_norm(input_, running_mean, running_var, weight=None, bias=None, training=True, momentum=0.1, eps=1e-5):
    use_batch_stats = training or running_mean is None or running_var is None
    if use_batch_stats:
        batch_mean = input_.mean(axis=(0, *range(2, input_.ndim)), keepdims=True)
        batch_var = input_.var(axis=(0, *range(2, input_.ndim)), keepdims=True)
        mean, var = batch_mean, batch_var
    else:
        mean = running_mean.reshape(1, C, *(1,) * spatial_dim)
        var = running_var.reshape(1, C, *(1,) * spatial_dim)

    normalized = (input_ - mean) / lucid.sqrt(var + eps)
    if weight is not None:
        normalized *= weight.reshape((1, C) + (1,) * spatial_dim)
    if bias is not None:
        normalized += bias.reshape((1, C) + (1,) * spatial_dim)
    return normalized
```

- **ì˜ì¡´ì„±**: `mean`, `var`, `sqrt`, `sub`, `div`, `mul`, `add` ë“± primitiveë§Œ ì‚¬ìš©.  
- **ìˆ˜ì‹**: $\hat{x} = (x - \mu) / \sqrt{\sigma^2 + \epsilon}$, $y = \gamma \hat{x} + \beta$  
- **ì„¤ê³„ ë©”ëª¨**: ëŸ¬ë‹ ìŠ¤íƒ¯ ì—…ë°ì´íŠ¸ëŠ” ëª¨ë“ˆì´ ì†Œìœ í•˜ì§€ë§Œ, ì—°ì‚° ìì²´ëŠ” primitive ì¡°í•©ìœ¼ë¡œ ëë‚¸ë‹¤. spatial ì°¨ì› ìˆ˜ë¥¼ ìë™ìœ¼ë¡œ ë§ì¶”ê¸° ìœ„í•´ reshape íŒ¨í„´ì„ ì¼ê´€ë˜ê²Œ ì ìš©í–ˆë‹¤.

### ğŸŒ§ï¸ `dropout`: ë§ˆìŠ¤í‚¹ê³¼ ìŠ¤ì¼€ì¼ë§

ê²½ë¡œ: [`lucid/nn/functional/_drop.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/nn/functional/_drop.py#L6-L40)

```python
def dropout(input_: Tensor, p: float = 0.5, training: bool = True) -> Tensor:
    if not training:
        return input_
    mask = (lucid.random.rand(*input_.shape) > p).free()
    scale = 1.0 / (1 - p)
    return input_ * mask * scale
```

- **ì˜ì¡´ì„±**: `random.rand`, `mul`, `sub` ë“± primitiveë§Œ ì‚¬ìš©.  
- **ìˆ˜ì‹**: $\mathrm{dropout}(x) = \dfrac{\mathbb{1}[u>p]}{1-p} \cdot x,\ u\sim\mathcal{U}(0,1)$  
- **ì„¤ê³„ ë©”ëª¨**: í•™ìŠµ/í‰ê°€ ëª¨ë“œë¥¼ ë¶„ë¦¬í•˜ê³ , maskë¥¼ `.free()`ë¡œ ê·¸ë˜í”„ì—ì„œ ë–¼ì–´ gradientê°€ í˜ëŸ¬ê°€ì§€ ì•Šê²Œ í–ˆë‹¤. spatial ì°¨ì›ì„ ë‹¨ì¼ ì¶•ìœ¼ë¡œ ë¬¶ëŠ” `dropoutnd`ë„ ë™ì¼í•œ íŒ¨í„´ìœ¼ë¡œ êµ¬í˜„í–ˆë‹¤.

### ğŸŠ `max_pool`: ìŠ¬ë¼ì´ìŠ¤ ì¬ì‚¬ìš©

ê²½ë¡œ: [`lucid/nn/functional/_pool.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/nn/functional/_pool.py#L93-L126)

```python
def max_pool2d(input_, kernel_size=1, stride=1, padding=0) -> Tensor:
    return lucid.max(_pool2d(input_, kernel_size, stride, padding), axis=-1)
```

- **ì˜ì¡´ì„±**: ë‚´ë¶€ `_pool2d`ëŠ” `pad`, `stack`, `slice` ì¡°í•©ë§Œ ì‚¬ìš©. ìµœì¢… ì¶•ì†ŒëŠ” primitive `max`.  
- **ìˆ˜ì‹**: ê° ìœˆë„ìš°ì—ì„œ $\max$ ì„ íƒ.  
- **ì„¤ê³„ ë©”ëª¨**: 1D/2D/3Dë¥¼ ë™ì¼ íŒ¨í„´ìœ¼ë¡œ êµ¬í˜„í•˜ê³ , ì»¤ë„/ìŠ¤íŠ¸ë¼ì´ë“œ/íŒ¨ë”©ì„ íŠœí”Œë¡œ ì •ê·œí™”í•´ ëª¨ë“ˆ(`nn.MaxPool2d`)ì´ ê·¸ëŒ€ë¡œ ìœ„ì„í•  ìˆ˜ ìˆë‹¤.

### ğŸ§­ `layer_norm`: ì¶• ê¸°ì¤€ ì •ê·œí™”

ê²½ë¡œ: [`lucid/nn/functional/_norm.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/nn/functional/_norm.py#L60-L104)

```python
def layer_norm(input_, normalized_shape, weight=None, bias=None, eps=1e-5):
    mean = input_.mean(axis=tuple(range(-len(normalized_shape), 0)), keepdims=True)
    var = input_.var(axis=tuple(range(-len(normalized_shape), 0)), keepdims=True)
    normalized = (input_ - mean) / lucid.sqrt(var + eps)
    if weight is not None:
        normalized *= weight.reshape((1,) * (input_.ndim - len(normalized_shape)) + normalized_shape)
    if bias is not None:
        normalized += bias.reshape((1,) * (input_.ndim - len(normalized_shape)) + normalized_shape)
    return normalized
```

- **ì˜ì¡´ì„±**: `mean`, `var`, `sqrt`, `sub`, `div`, `mul`, `add`.  
- **ìˆ˜ì‹**: ì…ë ¥ì˜ ë§ˆì§€ë§‰ `normalized_shape` ì¶•ë“¤ì— ëŒ€í•´ í‰ê· /ë¶„ì‚°ì„ êµ¬í•´ ì •ê·œí™” í›„ affine.  
- **ì„¤ê³„ ë©”ëª¨**: ë°°ì¹˜ í¬ê¸°ë‚˜ ì‹œí€€ìŠ¤ ê¸¸ì´ì— ìƒê´€ì—†ì´ ë™ì¼í•œ íŒ¨í„´ì„ ì ìš©í•˜ê¸° ìœ„í•´ reshape íŒ¨í„´ì„ ì¼ë°˜í™”í–ˆë‹¤.

### ğŸ¯ `cross_entropy`: ì•ˆì •í™”ëœ í™•ë¥  ë³€í™˜

ê²½ë¡œ: [`lucid/nn/functional/_loss.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/nn/functional/_loss.py#L52-L97)

```python
def cross_entropy(input_, target, weight=None, reduction="mean", eps=1e-7, ignore_index=None):
    exp_logits = lucid.exp(input_ - lucid.max(input_, axis=1, keepdims=True))
    prob = exp_logits / lucid.sum(exp_logits, axis=1, keepdims=True)
    loss = -lucid.log(prob[indices, target_int] + eps)
    ...
    return _loss_reduction_or_ignore(loss, weight, target_int, ignore_index, reduction)
```

- **ì˜ì¡´ì„±**: `max`, `exp`, `sum`, `truediv`, `log`, ì¸ë±ì‹± ë“± primitiveë§Œ ì‚¬ìš©.  
- **ìˆ˜ì‹**: $\mathrm{CE}(z, y) = -\log \dfrac{\exp(z_y)}{\sum_j \exp(z_j)}$ (ì•ˆì •í™”ë¥¼ ìœ„í•´ $z - \max z$ ì ìš©)  
- **ì„¤ê³„ ë©”ëª¨**: ì•ˆì •í™”(shift)ì™€ ë§ˆìŠ¤í‚¹(ignore_index)ì„ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì²˜ë¦¬í•´ ìƒìœ„ ëª¨ë“ˆì´ë‚˜ í•™ìŠµ ë£¨í”„ê°€ ë‹¨ìˆœí™”ëœë‹¤.

---

### ğŸ§µ ë§ˆë¬´ë¦¬ ë° ë‹¤ìŒ ë‹¨ê³„

`nn.functional`ì„ êµ¬ì„±í•˜ë©´ì„œ **primitive ë ˆì´ì–´ì— ëª¨ë“  ì €ìˆ˜ì¤€ ì˜ì¡´ì„±ì„ ëª°ì•„ë„£ëŠ” ê·œìœ¨**ì´ ì‘ë™í•˜ëŠ”ì§€ ê²€ì¦í–ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ:
- ìƒìœ„ APIëŠ” ì˜¤ì§ Lucid primitive í˜¸ì¶œë§Œìœ¼ë¡œ ì‘ì„±ë˜ì–´ **ì½”ë“œ ê²½ë¡œê°€ ì½ê¸° ì‰½ê³ ** í…ŒìŠ¤íŠ¸ ë²”ìœ„ë„ ì¢í˜€ì¡Œë‹¤.
- NumPy ì§ì ‘ í˜¸ì¶œì´ í•˜ìœ„ ì—°ì‚°ì—ë§Œ ì¡´ì¬í•˜ë¯€ë¡œ, backendë¥¼ êµì²´í•˜ê±°ë‚˜ í™•ì¥í•  ë•Œ ìˆ˜ì • ë²”ìœ„ê°€ ëª…í™•í•´ì§„ë‹¤.
- ëª¨ë“ˆ(`nn.Conv2d`, `nn.Linear`, `nn.LayerNorm`, `nn.Dropout`, `nn.MaxPool2d` ë“±)ì€ ìƒíƒœë§Œ ë³´ìœ í•˜ê³  forwardëŠ” `functional` í˜¸ì¶œë¡œ êµ¬ì„±ë˜ì–´ **ì—­í•  ë¶„ë¦¬**ê°€ ìœ ì§€ëœë‹¤.
- í…ŒìŠ¤íŠ¸ ë‹¨ìœ„ê°€ ëª…í™•í•´ì ¸ primitive â†’ functional â†’ module â†’ model ìˆœìœ¼ë¡œ **ì ì§„ì  ê²€ì¦**ì´ ê°€ëŠ¥í•˜ë‹¤.

ë‹¤ìŒ ê¸°ë¡ì—ì„œëŠ” (ë³„ë„ ë¬¸ì„œë¡œ ë¶„ë¦¬í•˜ëŠ”) ì»¨ë³¼ë£¨ì…˜ êµ¬í˜„ì„ ë¨¼ì € ë‹¤ë£¨ê³ , ì´ì–´ì„œ ì´ `functional` ë ˆì´ì–´ ìœ„ì— ì–¹íŒ `nn.Module` êµ¬í˜„ê³¼, ì´ë¥¼ í•©ì³ ê°„ë‹¨í•œ ëª¨ë¸(ì˜ˆ: LeNet)ê¹Œì§€ ë¹Œë“œí•˜ëŠ” íë¦„ì„ ì •ë¦¬í•  ì˜ˆì •ì´ë‹¤. ì´ë ‡ê²Œ ê³„ì¸µì ìœ¼ë¡œ ìŒ“ì€ ì¶”ìƒí™”ê°€ ì‹¤ì œ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì—ì„œë„ ì¼ê´€ì„±ì„ ìœ ì§€í•˜ëŠ”ì§€ ì ê²€í•˜ëŠ” ê²ƒì´ ëª©í‘œë‹¤.
