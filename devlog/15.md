## ğŸ›ï¸ ë‹¤ì–‘í•œ LR Scheduler êµ¬í˜„

ì´ë²ˆ ê°œë°œ ì¼ì§€ëŠ” Lucidì˜ **í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬**ë¥¼ ì´ë§ë¼í•´, ìˆ˜ì‹ê³¼ ì½”ë“œê°€ ì–´ë–»ê²Œ 1:1ë¡œ ëŒ€ì‘ë˜ëŠ”ì§€ ê¸´ í˜¸í¡ìœ¼ë¡œ í’€ì–´ë³¸ë‹¤. ì˜µí‹°ë§ˆì´ì €ê°€ íŒŒë¼ë¯¸í„°ë¥¼ ê°±ì‹ í•œë‹¤ë©´, ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ì‹œê°„ ì¶•ì—ì„œ í•™ìŠµë¥  ê¶¤ì ì„ ì„¤ê³„í•œë‹¤. PyTorchì™€ ë™ì¼í•œ ì‚¬ìš©ì„±ì„ ëª©í‘œë¡œ í•˜ë˜, LucidëŠ” ìµœì†Œí•œì˜ ìƒíƒœë¡œ ì§ë ¬í™”Â·ë³µì›ì„ ë‹¨ìˆœí™”í•˜ê³ , íŒŒë¼ë¯¸í„° ê·¸ë£¹/ë””ë°”ì´ìŠ¤ì™€ ì¶©ëŒ ì—†ì´ ë™ì‘í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆë‹¤.

---

### ğŸª¢ ê³µí†µ ë² ì´ìŠ¤ â€“ LRSchedulerì™€ ìƒíƒœ ì§ë ¬í™”

ëª¨ë“  ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” `LRScheduler`ë¥¼ ìƒì†í•œë‹¤([`lucid/optim/lr_scheduler/_base.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/optim/lr_scheduler/_base.py#L1-L85)). í•µì‹¬ í•„ë“œ:

- `base_lrs`: ì˜µí‹°ë§ˆì´ì € ê° íŒŒë¼ë¯¸í„° ê·¸ë£¹ì˜ ì´ˆê¸° lr ìŠ¤ëƒ…ìƒ·
- `last_epoch`: ë§ˆì§€ë§‰ìœ¼ë¡œ ì ìš©í•œ ì—í­(í˜¹ì€ í˜¸ì¶œ) ë²ˆí˜¸
- `_last_lr`: ì§ì „ ìŠ¤í…ì—ì„œ ì‹¤ì œë¡œ ì“´ lr ë¦¬ìŠ¤íŠ¸
- `_step_count`: `step()` í˜¸ì¶œ íšŸìˆ˜

ìƒíƒœ ì§ë ¬í™”ëŠ” ë‹¤ìŒ ë”•ì…”ë„ˆë¦¬ë¡œ í‘œí˜„ëœë‹¤:

$$
\text{state\_dict} = \{\text{last\_epoch},\, \text{base\_lrs},\, \_ \text{step\_count},\, \_ \text{last\_lr},\, \_ \text{group\_count}\}.
$$

`load_state_dict`ëŠ” ê·¸ë£¹ ìˆ˜ê°€ ë‹¤ë¥´ë©´ ì¦‰ì‹œ ì˜ˆì™¸ë¥¼ ë˜ì ¸ ì˜ëª»ëœ ë³µì›ì„ ì°¨ë‹¨í•œë‹¤. ì¦‰, **ëª¨ë¸Â·ì˜µí‹°ë§ˆì´ì € êµ¬ì„±ì´ ë°”ë€Œë©´ ìŠ¤ì¼€ì¤„ëŸ¬ë„ í•¨ê»˜ ì¬ìƒì„±**í•˜ëŠ” ê²ƒì´ ì•ˆì „í•˜ë‹¤. `step(epoch=None)`ì€ í˜¸ì¶œ ì‹œì ì„ ê¸°ì¤€ìœ¼ë¡œ `last_epoch`ë¥¼ ì¦ê°€ì‹œí‚¤ê±°ë‚˜ ì „ë‹¬ëœ epochë¡œ ì„¤ì •í•˜ê³ , `get_lr()`ê°€ ë°˜í™˜í•œ ë¦¬ìŠ¤íŠ¸ë¥¼ ê° ê·¸ë£¹ì˜ `lr`ì— ë®ì–´ì“´ë‹¤.

#### íŒŒë¼ë¯¸í„° ê·¸ë£¹ê³¼ ìŠ¤ì¼€ì¤„ ë™ê¸°í™”

ê·¸ë£¹ë³„ë¡œ ë‹¤ë¥¸ lrì„ ì“°ê³  ì‹¶ë‹¤ë©´ ì˜µí‹°ë§ˆì´ì €ë¥¼ ë§Œë“¤ ë•Œ ê·¸ë£¹ dictì— lrì„ ëª…ì‹œí•´ì•¼ í•œë‹¤. ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” `base_lrs`ì— ì €ì¥ëœ ê°’ì— ë™ì¼ factorë¥¼ ê³±í•˜ê±°ë‚˜, Noamì²˜ëŸ¼ ì ˆëŒ€ lrì„ ë°˜í™˜í•˜ëŠ” ê²½ìš° ëª¨ë“  ê·¸ë£¹ì— ê°™ì€ ê°’ì„ ì ìš©í•œë‹¤. ê·¸ë£¹ ìˆ˜ê°€ ë¶ˆì¼ì¹˜í•˜ë©´ `ValueError`ê°€ ë°œìƒí•˜ë¯€ë¡œ, ì²´í¬í¬ì¸íŠ¸ ì¬ê°œ ì‹œ ë™ì¼í•œ ê·¸ë£¹ êµ¬ì„±ì„ ìœ ì§€í•´ì•¼ í•œë‹¤.

### ğŸŒ€ Step ê³„ì—´ â€“ StepLR, MultiStepLR

**ìˆ˜ì‹**: ì¼ì • ì£¼ê¸°ë§ˆë‹¤ $\text{lr} \leftarrow \gamma \cdot \text{lr}$.

**êµ¬í˜„**: [`lucid/optim/lr_scheduler/_schedulers.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/optim/lr_scheduler/_schedulers.py#L12-L78)

- **StepLR**: $\text{factor} = \gamma^{\lfloor t / \text{step\_size} \rfloor}$, ìƒˆ lrì€ $\text{base\_lr} \times \text{factor}$.
- **MultiStepLR**: ë§ˆì¼ìŠ¤í†¤ $M$ì— ëŒ€í•´ $\text{factor} = \gamma^{\sum_{m \in M} \mathbb{1}[t \ge m]}$.

```python
factor = self.gamma ** (self.last_epoch // self.step_size)               # StepLR
factor = self.gamma ** sum(self.last_epoch >= m for m in milestones)     # MultiStepLR
new_lrs = [base_lr * factor for base_lr in self.base_lrs]
```

#### ì–¸ì œ ì“°ë‚˜?

ê°ì‡  ì‹œì ì„ ëª…í™•íˆ ì•Œê³  ìˆì„ ë•Œ(ì˜ˆ: 30/60/90 ì—í­). ì´ë¯¸ì§€ ë¶„ë¥˜ ì „í†µ ì„¤ì •ì—ì„œ ìì£¼ ì“°ì¸ë‹¤. `step_size`ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ê³¼ë„í•œ ê°ì‡ ë¡œ ì–¸ë”í”¼íŒ…, ë„ˆë¬´ ê¸¸ë©´ ìˆ˜ë ´ì´ ëŠë ¤ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ ì—í­ê³¼ ë°ì´í„°ì…‹ í¬ê¸°ì— ë§ì¶° ì¡°ì •í•œë‹¤.

#### ì‚¬ìš© ì˜ˆ

```python
optimizer = lucid.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
scheduler = lucid.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

for epoch in range(90):
    train_one_epoch(...)
    scheduler.step()
```

### â˜„ï¸ ExponentialLR â€“ ê¸°í•˜ê¸‰ìˆ˜ ê°ì‡ 

**ìˆ˜ì‹**: $\text{lr}_t = \text{base\_lr} \cdot \gamma^{t}$.

**êµ¬í˜„**: [`lucid/optim/lr_scheduler/_schedulers.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/optim/lr_scheduler/_schedulers.py#L80-L104)

`factor = self.gamma ** self.last_epoch`ë¡œ ê³„ì‚°í•˜ê³  ê·¸ë£¹ë³„ lrì„ ì¼ê´„ ê°±ì‹ í•œë‹¤. $\gamma > 0$ ê²€ì¦ìœ¼ë¡œ ìŒìˆ˜/0 í•™ìŠµë¥ ì„ ì°¨ë‹¨í•œë‹¤. ì—í­ì´ ë§ì„ìˆ˜ë¡ ì§€ìˆ˜ì  ê°ì‡ ê°€ ë¹ ë¥´ê²Œ ì§„í–‰ë˜ë¯€ë¡œ, ì‘ì€ $\gamma$ì¼ìˆ˜ë¡ ì¤‘ë°˜ ì´í›„ í•™ìŠµë¥ ì´ ê¸‰ê²©íˆ ë–¨ì–´ì§„ë‹¤ëŠ” ì ì„ ì—¼ë‘ì— ë‘”ë‹¤.

#### ì‚¬ìš© ì˜ˆ

```python
optimizer = lucid.optim.Adam(model.parameters(), lr=1e-3)
scheduler = lucid.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)

for epoch in range(50):
    train_one_epoch(...)
    scheduler.step()
```

### ğŸŒ— CosineAnnealingLR â€“ ì½”ì‚¬ì¸ ì§„í­ ê°ì†Œ

**ìˆ˜ì‹**:
$$
\text{lr}_t = \eta_{\min} + (\text{base\_lr} - \eta_{\min}) \cdot \frac{1 + \cos(\pi t / T_{\max})}{2}.
$$

**êµ¬í˜„**: [`lucid/optim/lr_scheduler/_schedulers.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/optim/lr_scheduler/_schedulers.py#L106-L137)

`T_max` ë™ì•ˆ ì ˆë°˜ ì£¼ê¸°ì˜ ì½”ì‚¬ì¸ìœ¼ë¡œ ë¶€ë“œëŸ½ê²Œ ê°ì†Œí•˜ë©°, $\eta_{\min}$ì— ë‹¿ëŠ”ë‹¤. `T_max <= 0`ì´ë©´ ì˜ˆì™¸ë¡œ ë§‰ëŠ”ë‹¤. ì½”ì‚¬ì¸ íŠ¹ì„±ìƒ ì´ˆë°˜ì—ëŠ” ì™„ë§Œí•˜ê²Œ ìœ ì§€ë˜ë‹¤ê°€ í›„ë°˜ì— ê¸‰ê²©íˆ ê°ì†Œí•´, ì›Œë°ì—… ì—†ì´ë„ í›„ë°˜ ì •ì œ(fine-tuning) íš¨ê³¼ë¥¼ ì–»ëŠ”ë‹¤.

```python
t = self.last_epoch
lr = eta_min + (base_lr - eta_min) * (1 + math.cos(math.pi * t / T_max)) / 2
```

**ì¶”ê°€ ë©”ëª¨**: ì›Œë°ì—… ë¦¬ìŠ¤íƒ€íŠ¸ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ì—¬ëŸ¬ ì£¼ê¸°ë¥¼ ì›í•˜ë©´ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì¬ìƒì„±í•˜ê±°ë‚˜ `T_max`ë¥¼ ì˜ê²Œ ë‚˜ëˆ„ì–´ ì‚¬ìš©í•œë‹¤. $\eta_{\min}$ì„ 0ë³´ë‹¤ í¬ê²Œ ë‘ë©´ ê³¼ë„í•œ ê°ì‡ ë¡œ í•™ìŠµì´ ë©ˆì¶”ëŠ” ê²ƒì„ ë°©ì§€í•  ìˆ˜ ìˆë‹¤.

### ğŸš¥ ReduceLROnPlateau â€“ í”Œë˜í†  ê°ì§€ í›„ ê°ì†Œ

**ìˆ˜ì‹ ê°œìš”**: ëª¨ë‹ˆí„° ì§€í‘œ $m_t$ê°€ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ $\text{lr} \leftarrow \max(\text{lr} \cdot \text{factor}, \text{min\_lr})$.

**êµ¬í˜„**: [`lucid/optim/lr_scheduler/_schedulers.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/optim/lr_scheduler/_schedulers.py#L139-L215)

- `mode`ê°€ `min`ì´ë©´ ì§€í‘œê°€ ê°ì†Œí•´ì•¼ ê°œì„ , `max`ë©´ ì¦ê°€í•´ì•¼ ê°œì„ .
- `threshold_mode`ê°€ `rel`ì´ë©´ ìƒëŒ€ ì„ê³„ê°’(ë¹„ìœ¨), `abs`ë©´ ì ˆëŒ€ ì„ê³„ê°’.
- `patience`ë¥¼ ë„˜ê²¨ ë‚˜ìœ epochê°€ ëˆ„ì ë˜ë©´ `_reduce_lr()` ì‹¤í–‰.
- `cooldown`ìœ¼ë¡œ ì—°ì† ê°ì†Œë¥¼ ì œí•œ, `eps`ë¡œ ì˜ë¯¸ ì—†ëŠ” ë³€í™”ëŠ” ë¬´ì‹œ.

```python
if self.num_bad_epochs > self.patience:
    new_lr = max(group["lr"] * self.factor, self.min_lr)
    if group["lr"] - new_lr > self.eps:
        group["lr"] = new_lr
```

#### ì‚¬ìš© ì˜ˆ

```python
optimizer = lucid.optim.AdamW(model.parameters(), lr=3e-4)
scheduler = lucid.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode="min", factor=0.2, patience=3, cooldown=1, min_lr=1e-6
)
for epoch in range(30):
    train_loss = train_one_epoch(...)
    val_loss = evaluate(...)
    scheduler.step(val_loss)  # ì§€í‘œ í•„ìˆ˜
```

í”Œë˜í†  ìŠ¤ì¼€ì¤„ëŸ¬ë§Œ `step(metrics)` ì¸ìë¥¼ ìš”êµ¬í•˜ë©°, `get_lr()`ëŠ” `_last_lr`ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜í•œë‹¤ëŠ” ì ì´ ë‹¤ë¥¸ ìŠ¤ì¼€ì¤„ëŸ¬ì™€ ë‹¤ë¥´ë‹¤.

### ğŸ¢ CyclicLR â€“ ì‚¼ê°/ì§€ìˆ˜ ëª¨ë“œ ì£¼ê¸°

**ìˆ˜ì‹**: base_lrì™€ max_lr ì‚¬ì´ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ì™•ë³µ. ìœ„ìƒ $x$ë¥¼ ì‚¬ìš©í•´
$$
\text{lr} = \text{base} + (\text{max}-\text{base}) \cdot \max(0, 1-x) \cdot \text{scale}(cycle).
$$

**êµ¬í˜„**: [`lucid/optim/lr_scheduler/_schedulers.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/optim/lr_scheduler/_schedulers.py#L217-L282)

```python
cycle = self.last_epoch // self.total_size
x = abs(self.last_epoch / self.step_size_up - 2 * cycle - 1)

if mode == "triangular":
    scale_factor = 1.0
elif mode == "triangular2":
    scale_factor = 1 / (2**cycle)
elif mode == "exp_range":
    scale_factor = self.gamma**self.last_epoch
else:
    scale_factor = self.scale_fn(cycle) if self.scale_fn else 1.0

lr = base_lr + (max_lr - base_lr) * max(0, 1 - x) * scale_factor
```

**ì‚¬ìš© ë©”ëª¨**:
- `cycle_momentum`ì€ ìë¦¬ë§Œ ìˆì§€ë§Œ ëª¨ë©˜í…€ ì¡°ì •ì€ ë¯¸êµ¬í˜„ â†’ í•™ìŠµë¥ ë§Œ ë³€í•œë‹¤.
- `step_size_up/down`ì„ ë‹¤ë¥´ê²Œ ì„¤ì •í•˜ë©´ ë¹„ëŒ€ì¹­ ì‚¼ê°í˜•ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤(ì§§ê²Œ ì˜¬ë¦¬ê³  ê¸¸ê²Œ ë‚´ë¦¬ê¸° ë“±).
- exp_range ëª¨ë“œì—ì„œ `gamma < 1`ì´ë©´ ë°˜ë³µë ìˆ˜ë¡ í”¼í¬ê°€ ì¤„ì–´ íƒìƒ‰ ê°•ë„ê°€ ê°ì†Œí•œë‹¤.

### ğŸ›°ï¸ NoamScheduler â€“ Transformer ì›Œë°ì—… ìŠ¤ì¼€ì¤„

**ìˆ˜ì‹** (Noam):
$$
\text{lr}(t) = \text{factor} \cdot d_{\text{model}}^{-0.5} \cdot \min\left(t^{-0.5},\; t \cdot \text{warmup}^{-1.5}\right).
$$

**êµ¬í˜„**: [`lucid/optim/lr_scheduler/_schedulers.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/optim/lr_scheduler/_schedulers.py#L284-L323)

- `model_size`, `warmup_steps`, `factor`ë¡œ ê²°ì •.
- `step_num = max(self.last_epoch, 1)`ë¡œ 0-division ë°©ì§€.
- **ì ˆëŒ€ lr**ì„ ë°˜í™˜í•˜ë¯€ë¡œ base_lrë¥¼ ë¬´ì‹œí•˜ê³  ëª¨ë“  ê·¸ë£¹ì— ë™ì¼ lrì„ ì ìš©í•œë‹¤.

```python
step_num = max(self.last_epoch, 1)
scale = self.factor * (self.model_size ** -0.5)

warmup_term = step_num * (self.warmup_steps ** -1.5)
decay_term = step_num ** -0.5

lr_factor = scale * min(decay_term, warmup_term)
return [lr_factor for _ in self.base_lrs]
```

#### ì¶”ê°€ ë©”ëª¨

warmup êµ¬ê°„ì—ì„œëŠ” ì„ í˜• ì¦ê°€, ì´í›„ $t^{-0.5}$ë¡œ ê°ì†Œí•´ í° ëª¨ë¸ì—ì„œë„ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆë‹¤. base_lrëŠ” ê´€ìŠµì ìœ¼ë¡œ 1.0ìœ¼ë¡œ ë‘ì–´ë„ ë¬´ë°©í•˜ë©°, `factor`ë¡œ ì „ì²´ ìŠ¤ì¼€ì¼ì„ ì¡°ì •í•œë‹¤.

---

### ğŸ§ª ì‚¬ìš© ì˜ˆ â€“ ìŠ¤ì¼€ì¤„ëŸ¬ ì¥ì°© í•™ìŠµ ë£¨í”„

```python
model = MyNet().to("gpu")
optimizer = lucid.optim.Adam(model.parameters(), lr=1e-3)
scheduler = lucid.optim.lr_scheduler.CosineAnnealingLR(
    optimizer, T_max=50, eta_min=1e-5
)

for epoch in range(50):
    for inputs, targets in loader:
        inputs, targets = inputs.to("gpu"), targets.to("gpu")
        optimizer.zero_grad()

        out = model(inputs)
        loss = F.cross_entropy(out, targets)
        loss.eval()

        loss.backward()
        optimizer.step()
        
    scheduler.step()  # ReduceLROnPlateauë§Œ metrics í•„ìš”
```

í”Œë˜í†  ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì“¸ ë•ŒëŠ” `scheduler.step(val_loss)`ì²˜ëŸ¼ ì§€í‘œë¥¼ ë„˜ê²¨ì•¼ í•˜ë©°, ë‹¤ë¥¸ ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” `step()` í˜¸ì¶œë§Œìœ¼ë¡œ ì¶©ë¶„í•˜ë‹¤. CyclicLR/Noamì²˜ëŸ¼ ìŠ¤í… ë‹¨ìœ„ë¡œ ë³€í•˜ëŠ” ìŠ¤ì¼€ì¤„ì€ ë°°ì¹˜ë§ˆë‹¤ `step()`ì„ í˜¸ì¶œí•˜ëŠ” íŒ¨í„´ì„ íƒí•  ìˆ˜ ìˆì§€ë§Œ, í˜¸ì¶œ íšŸìˆ˜ì™€ ì˜ë¯¸(ì—í­/ìŠ¤í…)ë¥¼ ì¼ê´€ë˜ê²Œ ìœ ì§€í•´ì•¼ í•œë‹¤.

### ğŸ§­ ë””ë°”ì´ìŠ¤Â·ì§ë ¬í™”Â·ì¬í˜„ì„± ì²´í¬ë¦¬ìŠ¤íŠ¸

- **ë””ë°”ì´ìŠ¤**: ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” lr ìŠ¤ì¹¼ë¼ë§Œ ì¡°ì •í•˜ë¯€ë¡œ ì¥ì¹˜ì— êµ¬ì• ë°›ì§€ ì•ŠëŠ”ë‹¤. ë‹¤ë§Œ ëª¨ë¸Â·ì˜µí‹°ë§ˆì´ì €ë¥¼ ì›í•˜ëŠ” ë””ë°”ì´ìŠ¤ë¡œ ì˜®ê¸´ ë’¤ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ìƒì„±í•˜ë©´ ê·¸ë£¹ ê²€ì¦ì´ í™•ì‹¤í•˜ë‹¤.
- **state_dict**: ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœì™€ ì˜µí‹°ë§ˆì´ì € ìƒíƒœë¥¼ í•¨ê»˜ ì €ì¥/ë¡œë“œí•´ì•¼ ë™ì¼í•œ lr ê¶¤ì ì„ ì¬í˜„í•  ìˆ˜ ìˆë‹¤. ê·¸ë£¹ ìˆ˜ê°€ ë°”ë€Œë©´ ë¡œë“œ ì‹œ ì˜ˆì™¸ê°€ ë‚œë‹¤.
- **step ìœ„ì¹˜**: ëŒ€ë¶€ë¶„ ì—í­ ëì—ì„œ í˜¸ì¶œí•˜ì§€ë§Œ, ìŠ¤í… ë‹¨ìœ„ ë³€í˜•(Cyclic, Noam)ì—ì„œëŠ” ë°°ì¹˜ë§ˆë‹¤ í˜¸ì¶œí•  ìˆ˜ë„ ìˆë‹¤. LucidëŠ” `last_epoch`ë¥¼ í˜¸ì¶œ íšŸìˆ˜ë¡œ í•´ì„í•˜ë¯€ë¡œ í•œ ë°©ì‹ìœ¼ë¡œ í†µì¼í•´ì•¼ í•œë‹¤.

---

### ğŸ§  ì •ë¦¬

- **LRScheduler ë² ì´ìŠ¤**: íŒŒë¼ë¯¸í„° ê·¸ë£¹ ê²€ì¦ê³¼ ìƒíƒœ ì§ë ¬í™”ë¡œ ì¬í˜„ì„±ì„ í™•ë³´í•œë‹¤.
- **Step/MultiStep/Exponential**: ê³„ë‹¨ì‹Â·ê¸°í•˜ì‹ ê°ì‡ , $\gamma$ì™€ ì£¼ê¸°ë¡œ ë‹¨ìˆœ ì œì–´.
- **CosineAnnealing**: ì™„ë§Œ-ê¸‰ê²© ê°ì†Œ íŒ¨í„´, $\eta_{\min}$ë¡œ ë°”ë‹¥ ì„¤ì •.
- **ReduceLROnPlateau**: ì§€í‘œ ê°œì„  ë¶€ì§„ ì‹œì—ë§Œ ê°ì†Œ, `patience`/`cooldown`/`eps`ë¡œ ë…¸ì´ì¦ˆë¥¼ ì–µì œ.
- **CyclicLR**: ì£¼ê¸°ì  ë¦¬ì…‹ìœ¼ë¡œ íƒìƒ‰ ê°•í™”, ì‚¼ê°Â·ì§€ìˆ˜ ëª¨ë“œ ì œê³µ.
- **NoamScheduler**: ì›Œë°ì—… í›„ $t^{-0.5}$ ê°ì†Œ, base_lr ë¬´ì‹œí•˜ê³  ì ˆëŒ€ lrì„ ë°˜í™˜.

ê³µí†µì ìœ¼ë¡œ ëª¨ë¸ì„ ì›í•˜ëŠ” ë””ë°”ì´ìŠ¤ë¡œ ì˜®ê¸´ ë’¤ ì˜µí‹°ë§ˆì´ì €ë¥¼ ë§Œë“¤ê³ , ê·¸ ë‹¤ìŒ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì—°ê²°í•˜ëŠ” ìˆœì„œë¥¼ ì§€í‚¤ë©´ ë””ë°”ì´ìŠ¤ ë¶ˆì¼ì¹˜ë¥¼ í”¼í•  ìˆ˜ ìˆë‹¤. ìˆ˜ì‹ì„ ê·¸ëŒ€ë¡œ ì˜®ê¸´ êµ¬í˜„ ë•ë¶„ì— íŒŒë¼ë¯¸í„°ë§Œ ë§ì¶”ë©´ ë°”ë¡œ ì‹¤í—˜ì— íˆ¬ì…í•  ìˆ˜ ìˆìœ¼ë©°, ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ë³µì›ë„ ê°„ê²°í•˜ë‹¤.
