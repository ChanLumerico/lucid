## ğŸ§® ì‹¤ì œ í…ì„œ ì—°ì‚°ë“¤ì˜ êµ¬í˜„

### ğŸ”” ì¶”ìƒí™”ê°€ ì½”ë“œ ê²½ë¡œë¥¼ ëœì–´ë‚¸ ë°©ì‹

ì§€ë‚œ ê¸°ë¡ì—ì„œ `@func_op`ê°€ forward/backwardì˜ ìˆ˜í•™ì  ì •ì˜ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ê·¸ë˜í”„ ì—°ê²°ì„ ìë™í™”í•˜ëŠ” êµ¬ì¡°ë¥¼ ì •ë¦¬í–ˆë‹¤. ì´ë²ˆì—ëŠ” ê·¸ í‹€ì„ ê¸°ë°˜ìœ¼ë¡œ `lucid/_func/bfunc.py`, `lucid/_func/ufunc.py`ì— ìˆëŠ” í•µì‹¬ ì—°ì‚°ë“¤ì´ ì–´ë–»ê²Œ ì‘ì„±ëëŠ”ì§€, **gradient ìˆ˜ì‹**ê³¼ **ì½”ë“œ ìŠ¤ë‹ˆí«**ì„ ì§ì§€ì–´ ê¸°ìˆ í•œë‹¤. ëª©í‘œëŠ” ë”¥ëŸ¬ë‹ ë£¨í”„ì—ì„œ ë°˜ë³µ í˜¸ì¶œë˜ëŠ” ê¸°ë³¸ ì—°ì‚°ì„ *ì¼ê´€ëœ í…œí”Œë¦¿* ìœ„ì— ì˜¬ë ¤ë†“ëŠ” ê²ƒì´ì—ˆê³ , ì‹¤ì œ êµ¬í˜„ì—ì„œ ê³ ë ¤í•œ broadcasting, shape ì¶•ì†Œ, CPU/GPU ë¶„ê¸° í¬ì¸íŠ¸ë¥¼ í•¨ê»˜ ì ëŠ”ë‹¤.

---

### ğŸ›ï¸ ê¸°ë³¸ ì—°ì‚° ì„ ì • ê¸°ì¤€

ì—°ì‚°ì„ ì¶”ë¦´ ë•ŒëŠ” ë‘ ê°€ì§€ ì¡°ê±´ì„ ê±¸ì—ˆë‹¤. 

1. **í•™ìŠµ ë£¨í”„ì—ì„œ ê±°ì˜ ë§¤ ìŠ¤í… í˜¸ì¶œë˜ëŠ”ê°€?**
2. **`@func_op`ê°€ ì²˜ë¦¬í•´ì•¼ í•  ê·¸ë˜í”„ í›„ì²˜ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ê°€?**

ì´ ì¡°ê±´ì„ ë§Œì¡±í•˜ë©´ì„œë„ ìˆ˜ì‹ì„ ê¹”ë”íˆ ì •ë¦¬í•  ìˆ˜ ìˆëŠ” ë‹¤ì„¯ ê°€ì§€ë¥¼ ê³¨ëë‹¤: `add`, `multiply`, `maximum`, `exp`Â·`log`(ìŒìœ¼ë¡œ), `matmul`. 

ì•„ë˜ëŠ” ê° ì—°ì‚°ì˜ í•µì‹¬ í¬ì¸íŠ¸ì™€ êµ¬í˜„ ì„¸ë¶€ ë‚´ìš©ì´ë‹¤.

### â• `add`: broadcastingê³¼ gradient ì •ë ¬

ë§ì…ˆì€ ê°„ë‹¨í•˜ì§€ë§Œ, ì…ë ¥ì´ ìŠ¤ì¹¼ë¼/í…ì„œ í˜¼í•©ì¼ ë•Œ **gradient shapeì„ ë§ì¶° ëŒë ¤ì£¼ëŠ”ì§€** í™•ì¸í•´ì•¼ í–ˆë‹¤. ë°ì½”ë ˆì´í„°ê°€ ì…ë ¥ ìºìŠ¤íŒ…ê³¼ ê·¸ë˜í”„ ì—°ê²°ì„ ì²˜ë¦¬í•˜ë¯€ë¡œ ìˆ˜ì‹ë§Œ ëª…ì‹œí•œë‹¤.

```python
# lucid/_func/bfunc.py
class add(operation):
    @binary_func_op()
    def cpu(self, a: Tensor, b: Tensor) -> _FuncOpReturnType:
        self.result = Tensor(a.data + b.data)
        return self.result, self.__grad__

    def __grad__(self) -> _GradFuncType:
        return self.result.grad, self.result.grad
```

- **ìˆ˜ì‹**: $y = a + b \Rightarrow \partial y/\partial a = 1,\ \partial y/\partial b = 1$  
- **ë™ì‘**: ë°˜í™˜ë˜ëŠ” gradient ë‘ ê°œ ëª¨ë‘ ë™ì¼í•˜ë©°, `_match_grad_shape`ê°€ ë‚´ë¶€ì—ì„œ broadcastingëœ ì¶•ì„ ë‹¤ì‹œ ì ‘ì–´ ë„£ëŠ”ë‹¤. ìŠ¤ì¹¼ë¼ ë§ì…ˆ, í–‰ë ¬+ë²¡í„° ë“± ë‹¤ì–‘í•œ ì¡°í•©ì„ ê°™ì€ ì½”ë“œë¡œ ì²˜ë¦¬í•œë‹¤.

### âœ–ï¸ `multiply`: ì…ë ¥ ê°’ì´ ê³§ ê¸°ìš¸ê¸°

ê³±ì…ˆì€ ì²´ì¸ ë£°ì˜ ê¸°ë³¸ ì‚¬ë¡€ë‹¤. forward ì‹œ ì…ë ¥ì„ ìº¡ì²˜í•´ ë‘ê³  backwardì—ì„œ ê·¸ëŒ€ë¡œ ê³±í•´ì¤€ë‹¤. `partial`ì„ ì¨ì„œ ì…ë ¥ì„ ì•ˆì „í•˜ê²Œ í´ë¡œì €ì— ë„˜ê²¼ë‹¤.

```python
# lucid/_func/bfunc.py
class multiply(operation):
    @binary_func_op()
    def cpu(self, a: Tensor, b: Tensor) -> _FuncOpReturnType:
        self.result = Tensor(a.data * b.data)
        return self.result, partial(self.__grad__, a=a, b=b)

    def __grad__(self, a: Tensor, b: Tensor) -> _GradFuncType:
        return b.data * self.result.grad, a.data * self.result.grad
```

- **ìˆ˜ì‹**: $y = a \times b \Rightarrow \partial y/\partial a = b,\ \partial y/\partial b = a$  
- **ë™ì‘**: gradientëŠ” ì…ë ¥ê°’ê³¼ ê²°ê³¼ gradientë¥¼ ê³±í•œ í˜•íƒœë¡œ ì „ë‹¬ëœë‹¤. ì…ë ¥ì´ broadcastëë‹¤ë©´ `_match_grad_shape`ê°€ ë’¤ì—ì„œ ì¶•ì„ ì••ì¶•í•´ **leaf í…ì„œì˜ shape**ì— ë§ì¶˜ë‹¤.

### ğŸ›¡ï¸ `maximum`: ë§ˆìŠ¤í¬ ê¸°ë°˜ ë¶„ê¸°

`maximum`ì€ ReLUì˜ ê¸°ë°˜ì´ ë˜ë¯€ë¡œ **ë¶€í˜¸ ë¶„ê¸° ë§ˆìŠ¤í¬**ê°€ ì •í™•í•´ì•¼ í•œë‹¤. forwardì—ì„œ ì‚¬ìš©í•œ ë§ˆìŠ¤í¬ë¥¼ backwardì—ì„œë„ ë™ì¼í•˜ê²Œ ì‚¬ìš©í•´ gradientë¥¼ ë¶„ë¦¬í•œë‹¤.

```python
# lucid/_func/bfunc.py
class maximum(operation):
    @binary_func_op()
    def cpu(self, a: Tensor, b: Tensor) -> _FuncOpReturnType:
        self.result = Tensor(np.maximum(a.data, b.data))
        return self.result, partial(self.__grad__, a=a, b=b)

    def __grad__(self, a: Tensor, b: Tensor) -> _GradFuncType:
        a_grad = (a.data >= b.data).astype(a.data.dtype)
        b_grad = (a.data < b.data).astype(b.data.dtype)
        return a_grad * self.result.grad, b_grad * self.result.grad
```

- **ìˆ˜ì‹**: $y = \max(a, b)$, $\partial y/\partial a = \mathbb{1}[a \ge b]$, $\partial y/\partial b = \mathbb{1}[a < b]$  
- **ë™ì‘**: ë¶€í˜¸ ë¹„êµ ê²°ê³¼ë¥¼ float ë§ˆìŠ¤í¬ë¡œ ë³€í™˜í•´ `result.grad`ì™€ ê³±í•œë‹¤. `ReLU(x) = \max(0, x)`ì—ì„  $a=0$ì´ ê³ ì •ë˜ë¯€ë¡œ ë™ì¼í•œ ë¶„ê¸° ë¡œì§ì´ ê·¸ëŒ€ë¡œ ì ìš©ëœë‹¤.

### ğŸŒ¡ï¸ `exp`ì™€ `log`: ìˆ˜ì¹˜ ì•ˆì •ì„±ì˜ ê¸°ì´ˆ

`exp`ì™€ `log`ëŠ” softmax, cross-entropyì˜ **ìˆ˜ì¹˜ ì•ˆì •ì„±**ì— ì§ì ‘ ì—°ê²°ëœë‹¤. unary í…œí”Œë¦¿ì„ ì‚¬ìš©í•´ ë¶ˆí•„ìš”í•œ ì¸ì ì²˜ë¦¬ë¥¼ ì œê±°í–ˆê³ , ì…ë ¥ì„ ìº¡ì²˜í•´ì•¼ í•˜ëŠ” ê²½ìš°ë§Œ partialì„ ì‚¬ìš©í–ˆë‹¤.

```python
# lucid/_func/ufunc.py
class exp(operation):
    @unary_func_op()
    def cpu(self, a: Tensor) -> _FuncOpReturnType:
        self.result = Tensor(np.exp(a.data))
        return self.result, self.__grad__

    def __grad__(self) -> _GradFuncType:
        return self.result.data * self.result.grad


class log(operation):
    @unary_func_op()
    def cpu(self, a: Tensor) -> _FuncOpReturnType:
        self.result = Tensor(np.log(a.data))
        return self.result, partial(self.__grad__, a=a)

    def __grad__(self, a: Tensor) -> _GradFuncType:
        return (1 / a.data) * self.result.grad
```

- **ìˆ˜ì‹**: $y = e^x \Rightarrow \partial y/\partial x = e^x$, $y = \log x \Rightarrow \partial y/\partial x = 1/x$  
- **ë™ì‘**: `exp`ëŠ” forward ê²°ê³¼ê°€ ê³§ ë¯¸ë¶„ ê³„ìˆ˜ê°€ ëœë‹¤. `log`ëŠ” ì…ë ¥ì„ ê¸°ì–µí•´ì•¼ í•˜ë¯€ë¡œ partialì„ í†µí•´ `a`ë¥¼ í´ë¡œì €ë¡œ ì „ë‹¬í•œë‹¤. ì´ ì¡°í•©ì´ softmaxì—ì„œ ì‚¬ìš©í•˜ëŠ” `input - max` ì•ˆì •í™” ê²½ë¡œì—ì„œ ê·¸ëŒ€ë¡œ ì“°ì¸ë‹¤.

### ğŸ§­ `matmul`: ë°°ì¹˜ ì°¨ì›ê³¼ ì¶•ì†Œ ì²˜ë¦¬

í–‰ë ¬ ê³±ì€ ì…ë ¥ ì°¨ì› ì •ë ¬, ë°°ì¹˜ broadcasting, ì¶•ì†Œë¥¼ ëª¨ë‘ í¬í•¨í•œë‹¤. ë™ì¼í•œ ìˆ˜ì‹ì„ CPUì™€ Metal GPU ëª¨ë‘ì— ì ìš©í•˜ê³ , shape ì¶•ì†ŒëŠ” helperë¡œ ë¶„ë¦¬í–ˆë‹¤.

```python
# lucid/_func/bfunc.py
class matmul(operation):
    @binary_func_op()
    def cpu(self, a: Tensor, b: Tensor) -> _FuncOpReturnType:
        out = np.matmul(a.data, b.data)
        self.result = Tensor(out)
        return self.result, partial(self.__grad__, a=a, b=b, lib_=np)

    def __grad__(self, a: Tensor, b: Tensor, lib_: ModuleType) -> _GradFuncType:
        grad = self.result.grad
        if grad.ndim == 0:
            grad = lib_.reshape(grad, (1, 1))

        grad_a = lib_.matmul(grad, lib_.swapaxes(b.data, -1, -2))
        grad_b = lib_.matmul(lib_.swapaxes(a.data, -1, -2), grad)

        grad_a = self._reduce_broadcast_shape(grad_a, a.shape, lib_)
        grad_b = self._reduce_broadcast_shape(grad_b, b.shape, lib_)
        return grad_a, grad_b
```

- **ìˆ˜ì‹** (2D): $Y = A B$, $\partial L/\partial A = (\partial L/\partial Y) B^\top$, $\partial L/\partial B = A^\top (\partial L/\partial Y)$  
- **ë™ì‘**: ì™„ì „ ì¶•ì†Œëœ ê²½ìš° `grad`ë¥¼ `(1, 1)`ë¡œ reshapeí•´ ì—°ì‚°ì„ í†µì¼í•œë‹¤. ë°°ì¹˜ ì°¨ì›ì€ ë™ì¼í•œ ìˆ˜ì‹ì„ ê·¸ëŒ€ë¡œ ì ìš©í•œ ë’¤ `_reduce_broadcast_shape`ê°€ í•„ìš” ì—†ëŠ” ì¶•ì„ `sum(axis=...)`ë¡œ ì œê±°í•´ ì›ë˜ ì…ë ¥ shapeì— ë§ì¶˜ë‹¤. backend ì°¨ì´ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì£¼ì…(`np` ë˜ëŠ” `mx`)ìœ¼ë¡œë§Œ êµ¬ë¶„í•œë‹¤.

---

### ğŸ§µ ì •ë¦¬

`@func_op`ë¥¼ ì ìš©í•œ ë’¤ ì—°ì‚° êµ¬í˜„ì€ **ìˆ˜í•™ì  ì •ì˜**ì™€ í•„ìš”í•œ **ìºì‹œ**ë§Œ ë‚¨ê³ , ê·¸ë˜í”„ ì—°ê²°ê³¼ broadcasting í›„ì²˜ë¦¬ëŠ” í…œí”Œë¦¿ì´ ì²˜ë¦¬í•˜ëŠ” í˜•íƒœë¡œ ì •ë¦¬ëë‹¤. ë•ë¶„ì—
- gradient ìˆ˜ì‹ì´ ì˜ëª»ëì„ ë•Œë§Œ ìˆ˜ì •í•˜ë©´ ë˜ê³ , ê·¸ë˜í”„ ë©”íƒ€ë°ì´í„°ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠëŠ”ë‹¤.
- CPU/GPU ë¶„ê¸°ëŠ” ë°ì½”ë ˆì´í„° ì¸ìë§Œ ë‹¤ë¥´ê²Œ ì£¼ë©´ ë˜ê³ , ì½”ë“œ ì¤‘ë³µ ì—†ì´ ìœ ì§€ëœë‹¤.
- `_match_grad_shape`ì™€ `_reduce_broadcast_shape`ê°€ ê³µí†µ ê²½ë¡œë¡œ ë¶™ì–´ ìˆì–´ shape ê´€ë ¨ ë²„ê·¸ë¥¼ ì¡°ê¸°ì— ì°¨ë‹¨í•œë‹¤.

ë‹¤ìŒ ë‹¨ê³„ì—ì„œëŠ” ì´ ê¸°ë°˜ ìœ„ì—ì„œ ì»¨ë³¼ë£¨ì…˜, padding, ê³ ì°¨ì› reduction ê°™ì€ ì—°ì‚°ì„ í™•ì¥í•˜ëŠ” ê³¼ì •ì„ ë‹¤ë£° ì˜ˆì •ì´ë‹¤. í•™ìŠµì— ì‹¤ì œë¡œ ì“°ì´ëŠ” ì—°ì‚°ê¹Œì§€ ë™ì¼í•œ í…œí”Œë¦¿ìœ¼ë¡œ ëŒì–´ì˜¬ ìˆ˜ ìˆëŠ”ì§€ ì ê²€í•˜ëŠ” ê²ƒì´ ëª©í‘œë‹¤.
