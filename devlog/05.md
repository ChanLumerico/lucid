## ğŸ§­ í…ì„œ ìœ í‹¸ë¦¬í‹° ì—°ì‚°ë“¤ì˜ êµ¬í˜„

### ğŸ§± ì¶”ìƒí™” ë§¥ë½

Lucidì˜ ì—°ì‚° ê³„ì¸µì„ ì—„ê²©íˆ ë‚˜ëˆŒ ë•Œ, ìµœí•˜ìœ„ primitive(`add`, `matmul` ë“±) ìœ„ì—ëŠ” **í˜•íƒœ ë³€í™˜Â·ì¶• ì¡°ì‘Â·ì§‘ê³„ ì—°ì‚°**ì´ ìˆë‹¤. ì´ë“¤ì€ `lucid._util.func`ì— ëª¨ì—¬ ìˆê³ , ì‹¤ì œë¡œëŠ” `reshape`, `squeeze`, `stack`, `pad`, `repeat`, `tile`, `flatten`, `broadcast_to`, `where`, `sort/topk` ê°™ì€ í•¨ìˆ˜ë“¤ì´ë‹¤. ì´ ì¸µì˜ ëª©í‘œëŠ” **NumPy í˜¸ì¶œì„ ì´ ë ˆì´ì–´ì—ì„œë§Œ ì†Œë¹„**í•˜ë©´ì„œë„, gradientê°€ ì •í™•íˆ íë¥´ë„ë¡ ì¶•/shape ì •ë³´ë¥¼ ì¹˜ë°€í•˜ê²Œ ë‹¤ë£¨ëŠ” ê²ƒì´ë‹¤. ì•„ë˜ì—ì„œëŠ” ìì£¼ ì“°ì´ëŠ” ì—°ì‚°ë“¤ì„ ê³¨ë¼ forward ì •ì˜ì™€ gradient ì²˜ë¦¬, ê·¸ë¦¬ê³  `axis`/`keepdims` ê·œì•½ì„ ì–´ë–»ê²Œ ë§ì·„ëŠ”ì§€ ì •ë¦¬í•œë‹¤.

---

### ğŸ”„ `reshape`/`squeeze`/`unsqueeze`: shape ì™•ë³µ ë³´ì¥

- **ê²½ë¡œ**: [`lucid/_util/func.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/_util/func.py#L24-L118)  
- **í•µì‹¬ í¬ì¸íŠ¸**: forwardì—ì„œ shapeë¥¼ ë³€ê²½í•˜ë©´ backwardì—ì„œ ì› shapeë¡œ **ì •í™•íˆ ë˜ëŒë¦¬ëŠ”** reshapeê°€ í•„ìš”í•˜ë‹¤.

```python
class reshape(operation):
    @unary_func_op()
    def cpu(self, a: Tensor):
        self.result = Tensor(a.data.reshape(self.shape))
        return self.result, partial(self.__grad__, a=a)

    def __grad__(self, a: Tensor):
        return self.result.grad.reshape(*a.shape)
```

`squeeze`ì™€ `unsqueeze`ë„ ë™ì¼í•œ íŒ¨í„´ì„ ë”°ë¥¸ë‹¤. `squeeze`ëŠ” íŠ¹ì • axisë¥¼ ì œê±°í•˜ê³  backwardì—ì„œ `reshape(a.shape)`ë¡œ ì›ë³µ, `unsqueeze`ëŠ” `expand_dims` í›„ backwardì—ì„œ `squeeze`ë¡œ ì¶•ì†Œí•œë‹¤. **ê·œì¹™**: shape ì •ë³´ëŠ” forwardì—ì„œ ìº¡ì²˜í•˜ê³ , backwardì—ì„œëŠ” *ë‹¨ì¼ reshape*ìœ¼ë¡œ ë³µì›í•œë‹¤. ì¶”ê°€ ë¡œì§ì´ë‚˜ ì¡°ê±´ ë¶„ê¸°ëŠ” ëª¨ë‘ forwardì—ë§Œ ë‘”ë‹¤.

---

### ğŸ“¦ `stack`/`concatenate`: ë¶„ë¦¬-ë³‘í•©ì˜ ìŒëŒ€ì„±

- **ê²½ë¡œ**: `stack`, `hstack`, `vstack`, `concatenate` ([`lucid/_util/func.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/_util/func.py#L121-L218))  
- **forward**: ì—¬ëŸ¬ Tensorë¥¼ íŠ¹ì • axisë¡œ í•©ì¹œë‹¤.  
- **backward**: í•©ì³ì§„ gradientë¥¼ ì…ë ¥ ê°œìˆ˜ë§Œí¼ **split** í•´ì„œ ë˜ëŒë¦°ë‹¤.

```python
class stack(operation):
    def __grad__(self, arr: tuple[Tensor], lib_: ModuleType):
        split_grads = lib_.split(self.result.grad, len(arr), axis=self.axis)
        return tuple(split_grads)

class concatenate(operation):
    def __grad__(self, arr: tuple[Tensor, ...]):
        split_sizes = [a.shape[self.axis] for a in arr]
        grad = self.result.grad
        outputs = []
        start = 0
        for size in split_sizes:
            slicer = [slice(None)] * grad.ndim
            slicer[self.axis] = slice(start, start + size)
            outputs.append(grad[tuple(slicer)])
            start += size
        return tuple(outputs)
```

**ì¶• ê·œì•½**: ëª¨ë“  ì…ë ¥ì˜ ndimê³¼ axisê°€ ì¼ì¹˜í•œë‹¤ê³  ê°€ì •í•˜ë©°, ë¸Œë¡œë“œìºìŠ¤íŠ¸ëŠ” í•˜ì§€ ì•ŠëŠ”ë‹¤. gradientëŠ” forwardì—ì„œ ìŒ“ì¸ ìˆœì„œë¥¼ **ê·¸ëŒ€ë¡œ split** í•˜ì—¬ ëŒë ¤ì¤€ë‹¤.

---

### ğŸ§± `pad`: íŒ¨ë”© êµ¬ê°„ ì˜ë¼ë‚´ê¸°

- **ê²½ë¡œ**: [`lucid/_util/func.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/_util/func.py#L221-L274)  
- **forward**: `np.pad`/`mx.pad` í˜¸ì¶œ ì „ pad_widthë¥¼ `(before, after)` íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ê·œí™”í•œë‹¤.  
- **backward**: íŒ¨ë”©ì„ ì œê±°í•œ sliceë§Œ ë‚¨ê¸´ë‹¤.

```python
def __grad__(self, a: Tensor, lib_: ModuleType):
    grad_input = lib_.zeros_like(a.data)
    slices = []
    for before, after in self.pad_with_norm:
        start = before
        end = -after if after != 0 else None
        slices.append(slice(start, end))
    grad_input = self.result.grad[tuple(slices)]
    return grad_input
```

**ê·œì¹™**: forwardì—ì„œ í™•ì¥í•œ ì˜ì—­ì„ backwardì—ì„œëŠ” ë²„ë¦°ë‹¤. pad_width ì •ê·œí™”ê°€ í•µì‹¬ì´ë¯€ë¡œ, ë‹¨ì¼ intÂ·ê¸¸ì´ 2 íŠœí”ŒÂ·ì¶•ë³„ íŠœí”Œ ëª¨ë‘ `(before, after)` ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ ì¬ì‚¬ìš©í•œë‹¤.

---

### ğŸ” `repeat`/`tile`: ì¶•ë³„ í™•ì¥ê³¼ ì¶•ì†Œ

- **ê²½ë¡œ**: `repeat`, `tile` ([`lucid/_util/func.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/_util/func.py#L276-L424))  
- **forward**: íŠ¹ì • axis ë˜ëŠ” ì „ì²´(flat)ì—ì„œ ìš”ì†Œë¥¼ ë°˜ë³µ.  
- **backward**: ë°˜ë³µëœ ìœ„ì¹˜ì˜ gradientë¥¼ **í•©ì‚°**í•´ ì›ë˜ ìœ„ì¹˜ë¡œ ì¶•ì†Œ.

`repeat`ì˜ backwardëŠ” axisê°€ ì—†ëŠ” ê²½ìš°(flat)ì™€ íŠ¹ì • axisì¸ ê²½ìš°ë¥¼ ë‚˜ëˆ  ì²˜ë¦¬í•œë‹¤. í•µì‹¬ì€ **output ì¸ë±ìŠ¤ â†’ input ì¸ë±ìŠ¤** ë§¤í•‘ì„ ë§Œë“¤ê³ , ê±°ê¸°ì— gradë¥¼ accumulate í•˜ëŠ” ê²ƒ.

```python
def __grad__(self, a: Tensor, lib_: ModuleType):
    if self.axis is None:
        output_indices = np.repeat(np.arange(input_size), repeats_arr)
        np.add.at(grad_input_flat, output_indices, grad_output_flat)
        ...
    else:
        output_indices_axis = np.repeat(input_indices_axis, repeats_arr, axis=axis_)
        idx = np.stack(np.meshgrid(..., indexing="ij"))
        idx[axis_] = output_indices_axis
        np.add.at(grad_input, tuple(idx), self.result.grad)
        ...
```

`tile`ì€ repeatê³¼ ìœ ì‚¬í•˜ì§€ë§Œ reps ë°°ì—´ì„ shape ì•ìª½ì— ë¼ì›Œë„£ì–´ reshape í›„ ì§ìˆ˜ ì¶•ì— ëŒ€í•´ `sum(axis=axes_to_sum)`ì„ ìˆ˜í–‰í•œë‹¤. **ì›ë¦¬**: forwardì—ì„œ í™•ì¥í•œ ì°¨ì› ìˆ˜ë§Œí¼ backwardì—ì„œ `sum`ìœ¼ë¡œ ì¶•ì†Œí•œë‹¤.

---

### ğŸ§® `flatten`: êµ¬ê°„ í•©ì¹˜ê¸°

- **ê²½ë¡œ**: [`lucid/_util/func.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/_util/func.py#L425-L455)  
- **forward**: `[start_axis, end_axis]` êµ¬ê°„ì„ í•˜ë‚˜ì˜ ì¶•ìœ¼ë¡œ ê³±í•´ í•©ì¹œë‹¤.  
- **backward**: ì €ì¥í•œ `original_shape`ë¡œ reshape.

```python
flat_axis = 1
for i in range(start, end + 1):
    flat_axis *= a.shape[i]
new_shape = a.shape[:start] + (flat_axis,) + a.shape[end + 1 :]
self.result = Tensor(a.data.reshape(new_shape))

def __grad__(self):
    return self.result.grad.reshape(self.original_shape)
```

**ì¶• ê·œì•½**: ìŒìˆ˜ axisë„ í—ˆìš©í•´ `start/end`ë¥¼ ì‹¤ì œ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•œë‹¤. ê³±ì…ˆ ìˆœì„œëŠ” forwardì—ì„œ í™•ì •í•˜ê³  backwardëŠ” ë‹¨ì¼ reshapeë§Œ ìˆ˜í–‰í•œë‹¤.

---

### ğŸŒ `broadcast_to`: í™•ì¥ â†’ ì¶•ì†Œ

- **ê²½ë¡œ**: [`lucid/_util/func.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/_util/func.py#L577-L606)  
- **forward**: ì§€ì • shapeìœ¼ë¡œ broadcast.  
- **backward**: broadcastëœ ì¶•ì„ `sum`ìœ¼ë¡œ ì¶•ì†Œí•˜ê³ , ì›ë˜ shapeë¡œ reshape.

```python
def __grad__(self):
    input_shape = self.original_shape
    ...
    for axis, (in_dim, out_dim) in enumerate(zip(input_shape, self.shape)):
        if in_dim == 1 and out_dim > 1:
            self.result.grad = self.result.grad.sum(axis=axis, keepdims=True)
    return self.result.grad.reshape(self.original_shape)
```

**ì›ì¹™**: forwardì—ì„œ ëŠ˜ì–´ë‚œ ì¶•(í¬ê¸° 1 â†’ n)ì€ backwardì—ì„œ `sum(axis)`ë¡œ ì ‘ì–´ ë„£ëŠ”ë‹¤. ndimì´ ë‹¬ë¼ì§„ ê²½ìš° ì•ìª½ì— `(1,)*diff`ë¥¼ ë¶™ì—¬ ì •ë ¬í•œ í›„ ê²€ì‚¬í•œë‹¤.

---

### ğŸ¯ `where`: ì¡°ê±´ ë¶„ê¸°ì™€ zero-grad

- **ê²½ë¡œ**: [`lucid/_util/func.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/_util/func.py#L1185-L1213)  
- **forward**: `np.where`/`mx.where`ë¡œ ì¡°ê±´ ë¶„ê¸°.  
- **backward**: `cond`ì—ëŠ” gradientë¥¼ í˜ë¦¬ì§€ ì•Šê³ , `a`/`b`ë¡œë§Œ ë¶„ê¸°í•´ ì „ë‹¬.

```python
def __grad__(self, lib_: ModuleType):
    cond = self.cond_.data
    grad = self.result.grad
    grad_cond = lib_.array(0.0)
    grad_a = lib_.where(cond, grad, 0)
    grad_b = lib_.where(lib_.logical_not(cond), grad, 0)
    return grad_cond, grad_a, grad_b
```

**ê·œì¹™**: ì¡°ê±´ í…ì„œì— ëŒ€í•´ì„œëŠ” ë¯¸ë¶„í•˜ì§€ ì•ŠëŠ”ë‹¤(í•­ìƒ 0). ë¶„ê¸°ëœ ì˜ì—­ì€ ë§ˆìŠ¤í¬ ì—°ì‚°ìœ¼ë¡œ ì „ë‹¬í•œë‹¤.

---

### ğŸ”¢ `topk`/`sort`: ì¸ë±ìŠ¤ ì—­ì •ë ¬

- **ê²½ë¡œ**: `sort`, `topk` ([`lucid/_util/func.py`](https://github.com/ChanLumerico/lucid/blob/main/lucid/_util/func.py#L905-L1087))  
- **forward**: ê°’ê³¼ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜.  
- **backward**: ì¶œë ¥ gradientë¥¼ **ì›ë˜ ì¸ë±ìŠ¤ ìˆœì„œë¡œ ë˜ëŒë¦° ë’¤** ì…ë ¥ ìœ„ì¹˜ì— scatter/add.

```python
def __grad__(self, lib_):
    grad = self.result[0].grad
    reverse_indices = lib_.argsort(self.result[1].data, axis=self.axis)
    grad_out = lib_.take_along_axis(grad, reverse_indices, axis=self.axis)
    return grad_out
```

`topk`ëŠ” `indices`ì— ë”°ë¼ `np.put_along_axis`ë¡œ scatterí•œë‹¤. í•µì‹¬ì€ â€œì •ë ¬/ì„ íƒâ€ì˜ ì—­ì—°ì‚°ì„ gradient ê²½ë¡œì— ë§ì¶° êµ¬í˜„í•˜ëŠ” ê²ƒ.

---

### ğŸ§¾ ì§‘ê³„ ì—°ì‚°ê³¼ `keepdims`

`mean`, `sum`, `var` ë“± ì§‘ê³„ ì—°ì‚°ì€ reduction ì¶•ê³¼ `keepdims`ì— ë”°ë¼ gradient shapeì„ ë§ì¶”ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. (ì½”ë“œëŠ” `lucid/_tensor/tensor.py`ì™€ `_util`ì˜ reduce ê²½ë¡œì— ë¶„í¬.) 

ì¼ë°˜ ê·œì¹™:
- **forward**: ì„ íƒí•œ axisë¥¼ ì œê±°í•˜ê±°ë‚˜ `keepdims=True`ì¼ ê²½ìš° 1ë¡œ ìœ ì§€.  
- **backward**:  
  - reductionëœ ì¶•ì´ ì‚¬ë¼ì¡Œë‹¤ë©´ `reshape(..., 1, ...)` í›„ `broadcast_to`ë¡œ ì…ë ¥ shapeë¡œ í™•ì¥.  
  - `mean`ì€ ì¶”ê°€ë¡œ `1 / reduce_size` ìŠ¤ì¼€ì¼ì„ ê³±í•œë‹¤.  
  - `var`ëŠ” $(x-\mu)$ì— ëŒ€í•´ $2/N$ ìŠ¤ì¼€ì¼ì„ ê³±í•˜ê³ , í•„ìš”ì‹œ `keepdims` ì²˜ë¦¬ í›„ broadcast.

ì´ ë¡œì§ì€ `_match_grad_shape`ì™€ ë™ì¼í•œ ì² í•™ì„ ë”°ë¥¸ë‹¤: **ì¶•ì„ ì—†ì•´ë‹¤ë©´ backwardì—ì„œ ì¶•ì„ ë‹¤ì‹œ ë§Œë“¤ì–´ broadcast, í¬ê¸°ë¥¼ í‚¤ì› ë‹¤ë©´ sumìœ¼ë¡œ ì ‘ëŠ”ë‹¤.**

---

### ğŸ§µ ë§ˆë¬´ë¦¬

`lucid._util`ì˜ ìœ í‹¸ë¦¬í‹° ì—°ì‚°ì€ ëª¨ë¸ ì½”ë“œì—ì„œ ìì£¼ ë“±ì¥í•˜ì§€ë§Œ, ì‹¤ì œë¡œëŠ” **shape/axis bookkeeping**ê³¼ **gradient ì—­ì „**ì„ ì •í™•íˆ ì²˜ë¦¬í•˜ëŠ” ì‘ì€ ê·œì•½ ëª¨ìŒì´ë‹¤. ëª¨ë“  í•¨ìˆ˜ê°€ NumPy í˜¸ì¶œì„ ì´ ë ˆì´ì–´ì— ê°€ë‘ê³ , backwardì—ì„œ ì¶•/shapeë¥¼ ì™•ë³µì‹œì¼œì£¼ëŠ” íŒ¨í„´ì„ ê³µìœ í•œë‹¤. ë‹¤ìŒ ë¬¸ì„œì—ì„œëŠ” ì´ ìœ„ì— ìŒ“ì¸ `nn.functional` ê³„ì¸µ(í™œì„±í™”, ì •ê·œí™”, ì†ì‹¤ ë“±)ì„ ì •ë¦¬í•˜ê³ , ì»¨ë³¼ë£¨ì…˜ì€ ë³„ë„ë¡œ ìƒì„¸íˆ ë‹¤ë£° ì˜ˆì •ì´ë‹¤.
