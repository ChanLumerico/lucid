lucid.models
============

The `lucid.models` package provides a collection of predefined neural network 
architectures that are ready to use for various tasks, such as image classification 
and feature extraction. These models are designed to demonstrate key deep learning 
concepts while leveraging the modular and educational nature of the `lucid` framework.

.. list-table::
    :header-rows: 1
    :align: center

    * - Architecture
      - Description
      - Models
    
    * - ConvNets
      - Convolutional Neural Networks (ConvNets) are deep learning models designed 
        for processing grid-like data (e.g., images) using convolutional layers to 
        extract spatial features efficiently.
      - `ConvNets <conv/index.rst>`_

----

Transformers
------------

.. rubric:: Transformer

The Transformer is a deep learning architecture introduced by Vaswani et al. in 2017, 
designed for handling sequential data with self-attention mechanisms. It replaces 
traditional recurrent layers with attention-based mechanisms, enabling highly 
parallelized training and capturing long-range dependencies effectively.

 Vaswani, Ashish, et al. "Attention Is All You Need." 
 *Advances in Neural Information Processing Systems*, 2017, pp. 5998-6008.

.. list-table::
    :header-rows: 1
    :align: left

    * - Name
      - Model
      - Input Shape
      - Parameter Count
      - Implemented
    
    * - Transformer-Base
      - `transformer_base <transformer/base/transformer_base>`_
      - :math:`(N, L_{src})`, :math:`(N, L_{tgt})`
      - 62,584,544
      - âœ…
    
    * - Transformer-Big
      - `transformer_big <transformer/base/transformer_big>`_
      - :math:`(N, L_{src})`, :math:`(N, L_{tgt})`
      - 213,237,472
      - âœ…

.. rubric:: Visual Transformer (ViT)

The Vision Transformer (ViT) is a deep learning architecture introduced by 
Dosovitskiy et al. in 2020, designed for image recognition tasks using self-attention 
mechanisms. Unlike traditional convolutional neural networks (CNNs), ViT splits an 
image into fixed-size patches, processes them as a sequence, and applies Transformer 
layers to capture global dependencies.

 Dosovitskiy, Alexey, et al. "An Image is Worth 16x16 Words: Transformers for 
 Image Recognition at Scale." *International Conference on Learning Representations* 
 (ICLR), 2020.

.. list-table::
    :header-rows: 1
    :align: left

    * - Name
      - Model
      - Input Shape
      - Parameter Count
      - Implemented
    
    * - ViT-Ti
      - `vit_tiny <transformer/vit/vit_tiny>`_
      - :math:`(N,3,224,224)`
      - 5,717,416
      - âœ…
    
    * - ViT-S
      - `vit_small <transformer/vit/vit_small>`_
      - :math:`(N,3,224,224)`
      - 22,050,664
      - âœ…
    
    * - ViT-B
      - `vit_base <transformer/vit/vit_base>`_
      - :math:`(N,3,224,224)`
      - 86,567,656
      - âœ…
    
    * - ViT-L
      - `vit_large <transformer/vit/vit_large>`_
      - :math:`(N,3,224,224)`
      - 304,326,632
      - âœ…
    
    * - ViT-H
      - `vit_huge <transformer/vit/vit_huge>`_
      - :math:`(N,3,224,224)`
      - 632,199,400
      - âœ…

.. rubric:: Swin Transformer

The Swin Transformer is a hierarchical vision transformer introduced by 
Liu et al. in 2021, designed for image recognition and dense prediction 
tasks using self-attention mechanisms within shifted local windows. 
Unlike traditional convolutional neural networks (CNNs) and the original 
Vision Transformer (ViT)â€”which splits an image into fixed-size patches 
and processes them as a flat sequenceâ€”the Swin Transformer divides the 
image into non-overlapping local windows and computes self-attention 
within each window.

 *Swin Transformer*

 Liu, Ze, et al. "Swin Transformer: Hierarchical Vision Transformer using 
 Shifted Windows." arXiv preprint arXiv:2103.14030 (2021).

 *Swin Transformer-v2*

 Liu, Ze, et al. "Swin Transformer V2: Scaling Up Capacity and Resolution." 
 arXiv preprint arXiv:2111.09883 (2021).

.. list-table::
    :header-rows: 1
    :align: left

    * - Name
      - Model
      - Input Shape
      - Parameter Count
      - Implemented
    
    * - Swin-T
      - `swin_tiny <transformer/swin/swin_tiny>`_
      - :math:`(N,3,224,224)`
      - 28,288,354
      - âœ…
    
    * - Swin-S
      - `swin_small <transformer/swin/swin_small>`_
      - :math:`(N,3,224,224)`
      - 49,606,258
      - âœ…
    
    * - Swin-B
      - `swin_base <transformer/swin/swin_base>`_
      - :math:`(N,3,224,224)`
      - 87,768,224
      - âœ…
    
    * - Swin-L
      - `swin_large <transformer/swin/swin_large>`_
      - :math:`(N,3,224,224)`
      - 196,532,476
      - âœ…

.. list-table::
    :header-rows: 1
    :align: left

    * - Name
      - Model
      - Input Shape
      - Parameter Count
      - Implemented
    
    * - Swin-v2-T
      - `swin_v2_tiny <transformer/swin/swin_v2_tiny>`_
      - :math:`(N,3,224,224)`
      - 28,349,842
      - âœ…
    
    * - Swin-v2-S
      - `swin_v2_small <transformer/swin/swin_v2_small>`_
      - :math:`(N,3,224,224)`
      - 49,731,106
      - âœ…
    
    * - Swin-v2-B
      - `swin_v2_base <transformer/swin/swin_v2_base>`_
      - :math:`(N,3,224,224)`
      - 87,922,400
      - âœ…
    
    * - Swin-v2-L
      - `swin_v2_large <transformer/swin/swin_v2_large>`_
      - :math:`(N,3,224,224)`
      - 196,745,308
      - âœ…
    
    * - Swin-v2-H
      - `swin_v2_huge <transformer/swin/swin_v2_huge>`_
      - :math:`(N,3,224,224)`
      - 657,796,668
      - âœ…
    
    * - Swin-v2-G
      - `swin_v2_giant <transformer/swin/swin_v2_giant>`_
      - :math:`(N,3,224,224)`
      - 3,000,869,564
      - âœ…

.. rubric:: Convolutional Transformer (CvT)

CvT (Convolutional Vision Transformer) combines self-attention with depthwise 
convolutions to improve local feature extraction and computational efficiency. 
This hybrid design retains the global modeling capabilities of Vision Transformers 
while enhancing inductive biases, making it effective for image classification and 
dense prediction tasks.

 Wu, Haiping, et al. "CvT: Introducing Convolutions to Vision Transformers." 
 *Proceedings of the IEEE/CVF International Conference on Computer Vision*, 
 2021, pp. 22-31.

.. list-table::
    :header-rows: 1
    :align: left

    * - Name
      - Model
      - Input Shape
      - Parameter Count
      - Implemented
    
    * - CvT-13
      - `cvt_13 <transformer/cvt/cvt_13>`_
      - :math:`(N,3,224,224)`
      - 19,997,480
      - âœ…
    
    * - CvT-21
      - `cvt_21 <transformer/cvt/cvt_21>`_
      - :math:`(N,3,224,224)`
      - 31,622,696
      - âœ…
    
    * - CvT-W24
      - `cvt_w24 <transformer/cvt/cvt_w24>`_
      - :math:`(N,3,384,384)`
      - 277,196,392
      - âœ…

.. rubric:: Pyramid Vision Transformer (PVT)

*To be implemented...ðŸ”®*
